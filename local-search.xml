<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>test.md</title>
    <link href="/2025/08/02/test-md/"/>
    <url>/2025/08/02/test-md/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>拉取向量模型命令</title>
    <link href="/2025/07/15/%E2%80%9C%E6%8B%89%E5%8F%96%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B%E5%91%BD%E4%BB%A4%E2%80%9D/"/>
    <url>/2025/07/15/%E2%80%9C%E6%8B%89%E5%8F%96%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B%E5%91%BD%E4%BB%A4%E2%80%9D/</url>
    
    <content type="html"><![CDATA[<p>由于京东云服务器在每次重启会清空系统盘，因此每次重启服务器后需要手动拉取向量模型，执行如下命令:</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama pull nomic<span class="literal">-embed-text</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>大模型应用开发学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/12/16/Pytorch%E6%89%A9%E5%B1%95%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93/"/>
    <url>/2024/12/16/Pytorch%E6%89%A9%E5%B1%95%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h1 id="Pytorch扩展方式总结"><a href="#Pytorch扩展方式总结" class="headerlink" title="Pytorch扩展方式总结"></a>Pytorch扩展方式总结</h1><h2 id="1-使用-torch-提供的-Python-API"><a href="#1-使用-torch-提供的-Python-API" class="headerlink" title="1. 使用 torch 提供的 Python API"></a>1. 使用 <code>torch</code> 提供的 Python API</h2><p>该方式仅依赖 PyTorch 的现有功能，进行小规模扩展，不需要自定义底层操作。例如我们通过 PyTorch 的现有模块，如 <code>torch.nn.Module</code> 或函数操作，创建新功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(input_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.relu(<span class="variable language_">self</span>.linear(x))</span><br><span class="line"></span><br><span class="line">layer = CustomLayer(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">output = layer(torch.randn(<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我们也可以集成torch.autograd.Function，来进行自定义的前向传播和后向传播操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyFunction</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span></span>):</span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span> ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        <span class="built_in">input</span>, = ctx.saved_tensors</span><br><span class="line">        <span class="keyword">return</span> grad_output * <span class="number">2</span> * <span class="built_in">input</span></span><br><span class="line"></span><br><span class="line">custom_op = MyFunction.apply</span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = custom_op(x)</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2-用-TORCH-LIBRARY-宏注册-C-cuda扩展"><a href="#2-用-TORCH-LIBRARY-宏注册-C-cuda扩展" class="headerlink" title="2.用 TORCH_LIBRARY 宏注册 C++ &#x2F;cuda扩展"></a>2.用 <code>TORCH_LIBRARY</code> 宏注册 C++ &#x2F;cuda扩展</h2><p>该方式尝试用于需要注册多个底层操作，且提供对设备和数据类型的多重支持。</p><p>例如：用 <code>TORCH_LIBRARY</code> 宏注册操作，结合 <code>TORCH_LIBRARY_IMPL</code> 实现设备分发。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">custom_op_cpu</span><span class="params">(torch::Tensor input)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> input + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">custom_op_cuda</span><span class="params">(torch::Tensor input)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> input + <span class="number">2</span>;  <span class="comment">// CUDA 实现</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TORCH_LIBRARY</span>(custom_ops, m) &#123;</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;custom_op&quot;</span>, custom_op_cpu);  <span class="comment">// 默认 CPU 实现</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TORCH_LIBRARY_IMPL</span>(custom_ops, CUDA, m) &#123;</span><br><span class="line">    m.<span class="built_in">impl</span>(<span class="string">&quot;custom_op&quot;</span>, custom_op_cuda);  <span class="comment">// CUDA 实现</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>接下来简单介绍一下pytorch提供的常用于扩展的宏：</p><h3 id="2-1-TORCH-LIBRARY"><a href="#2-1-TORCH-LIBRARY" class="headerlink" title="2.1 TORCH_LIBRARY"></a>2.1 TORCH_LIBRARY</h3><p>用于定义一个新的操作库，注册与库名称相关的操作。通常是扩展的入口点。</p><h4 id="用法："><a href="#用法：" class="headerlink" title="用法："></a>用法：</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TORCH_LIBRARY</span>(pyg, m) &#123;</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;segment_matmul(Tensor input, Tensor ptr, Tensor other) -&gt; Tensor&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h4><ul><li><code>pyg</code> 是库的名称。</li><li><code>m.def</code> 用于注册操作的接口（签名），这些签名对应 Python 中的 <code>torch.ops.pyg.segment_matmul</code>。</li><li>该宏会生成全局的库管理器，其他设备实现或扩展都会与此库关联。</li></ul><h3 id="2-2-TORCH-LIBRARY-IMPL"><a href="#2-2-TORCH-LIBRARY-IMPL" class="headerlink" title="2.2 TORCH_LIBRARY_IMPL"></a>2.2 TORCH_LIBRARY_IMPL</h3><p>用于为特定设备（如 CPU 或 CUDA）实现上述注册的操作。</p><h4 id="用法：-1"><a href="#用法：-1" class="headerlink" title="用法："></a>用法：</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TORCH_LIBRARY_IMPL</span>(pyg, CUDA, m) &#123;</span><br><span class="line">    m.<span class="built_in">impl</span>(<span class="string">&quot;segment_matmul&quot;</span>, <span class="built_in">TORCH_FN</span>(segment_matmul_kernel));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="说明：-1"><a href="#说明：-1" class="headerlink" title="说明："></a>说明：</h4><ul><li><code>pyg</code>：与之前定义的库名称保持一致。</li><li><code>CUDA</code>：指定设备实现，可用的设备包括 <code>CPU</code>, <code>CUDA</code>, <code>HIP</code> 等。</li><li><code>m.impl</code>：注册特定设备的实现。</li><li><code>TORCH_FN</code>：将实现的函数（如 <code>segment_matmul_kernel</code>）封装为通用接口</li></ul><h3 id="2-3-TORCH-FN"><a href="#2-3-TORCH-FN" class="headerlink" title="2.3 TORCH_FN"></a>2.3 TORCH_FN</h3><p>将 C++ 函数指针转换为 PyTorch 注册系统可接受的形式。</p><h4 id="用法：-2"><a href="#用法：-2" class="headerlink" title="用法："></a>用法：</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m.<span class="built_in">impl</span>(<span class="string">&quot;segment_matmul&quot;</span>, <span class="built_in">TORCH_FN</span>(segment_matmul_kernel));</span><br></pre></td></tr></table></figure><h4 id="说明：-2"><a href="#说明：-2" class="headerlink" title="说明："></a>说明：</h4><ul><li>适用于无状态的函数注册，简化了函数绑定过程。</li><li>如果需要绑定具有上下文的类成员函数，可使用其他形式（如 <code>std::bind</code>）。</li></ul><h3 id="2-4-TORCH-LIBRARY-FRAGMENT"><a href="#2-4-TORCH-LIBRARY-FRAGMENT" class="headerlink" title="2.4 TORCH_LIBRARY_FRAGMENT"></a>2.4 TORCH_LIBRARY_FRAGMENT</h3><p>允许在不同的文件中对同一个库的扩展进行分段实现。</p><h4 id="用法：-3"><a href="#用法：-3" class="headerlink" title="用法："></a>用法：</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TORCH_LIBRARY_FRAGMENT</span>(pyg, m) &#123;</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;grouped_matmul(Tensor[] inputs, Tensor[] others, Tensor[] biases) -&gt; Tensor[]&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="说明：-3"><a href="#说明：-3" class="headerlink" title="说明："></a>说明：</h4><ul><li><code>TORCH_LIBRARY_FRAGMENT</code> 不会重复创建库，而是扩展现有库。</li><li>非常适合分模块开发。</li></ul><h2 id="3-使用-pybind11-直接扩展-C-CUDA"><a href="#3-使用-pybind11-直接扩展-C-CUDA" class="headerlink" title="3. 使用 pybind11 直接扩展 C++&#x2F;CUDA"></a>3. 使用 <code>pybind11</code> 直接扩展 C++&#x2F;CUDA</h2><h3 id="适用场景："><a href="#适用场景：" class="headerlink" title="适用场景："></a>适用场景：</h3><ul><li>更底层的扩展需求。</li><li>不需要与 PyTorch 的张量操作深度绑定。</li></ul><h3 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h3><p>通过 <code>pybind11</code> 将 C++ 方法导出为 Python 模块，并手动管理张量操作。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;pybind11/pybind11.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">add</span><span class="params">(<span class="type">float</span> a, <span class="type">float</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(custom_pybind, m) &#123;</span><br><span class="line">    m.<span class="built_in">def</span>(<span class="string">&quot;add&quot;</span>, &amp;add, <span class="string">&quot;Add two numbers&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> custom_pybind</span><br><span class="line"><span class="built_in">print</span>(custom_pybind.add(<span class="number">1.0</span>, <span class="number">2.0</span>))  <span class="comment"># 输出: 3.0</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>pyg-lib调用分析</title>
    <link href="/2024/12/11/pyg-lib%E8%B0%83%E7%94%A8%E5%88%86%E6%9E%90/"/>
    <url>/2024/12/11/pyg-lib%E8%B0%83%E7%94%A8%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h2 id="1-概要"><a href="#1-概要" class="headerlink" title="1. 概要"></a>1. 概要</h2><p>该文档主要对pyg-lib从python顶层至c++&#x2F;cuda底层的调用链进行分析。并分析了setup.py以及CmakeLists.txt以了解整个项目的编译以及安装过程。</p><p><img src="/pyg-lib%E8%B0%83%E7%94%A8%E9%93%BE.jpg" alt="pyg-lib调用链"></p><p>以上为pyg-lib由顶层至底层的调用链，在第二节以例子的形式进行分析。</p><h2 id="2-函数调用分析"><a href="#2-函数调用分析" class="headerlink" title="2. 函数调用分析"></a>2. 函数调用分析</h2><h3 id="2-1-index-sort"><a href="#2-1-index-sort" class="headerlink" title="2.1 index_sort()"></a>2.1 index_sort()</h3><p>index_sort()实现了一个高效的排序算法，在顶层部分直接调用由底层c++注册的操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> inputs.is_cpu:</span><br><span class="line">    <span class="keyword">return</span> torch.sort(inputs)</span><br><span class="line"><span class="keyword">return</span> torch.ops.pyg.index_sort(inputs, max_value)</span><br></pre></td></tr></table></figure><p>由于只提供了cpu加速，因此该函数不涉及cuda代码。其中ops为pyg自定义的操作空间，c++代码的定义在csrc目录下。</p><h4 id="2-1-1-index-sort-cpp"><a href="#2-1-1-index-sort-cpp" class="headerlink" title="2.1.1 index_sort.cpp"></a>2.1.1 index_sort.cpp</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;ATen/core/dispatch/Dispatcher.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/library.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> pyg &#123;</span><br><span class="line"><span class="keyword">namespace</span> ops &#123;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TORCH_LIBRARY_FRAGMENT</span>(pyg, m) &#123;</span><br><span class="line">  m.<span class="built_in">def</span>(<span class="built_in">TORCH_SELECTIVE_SCHEMA</span>(</span><br><span class="line">      <span class="string">&quot;pyg::index_sort(Tensor indices, int? max = None) -&gt; (Tensor, Tensor)&quot;</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;  <span class="comment">// namespace ops</span></span><br><span class="line">&#125;  <span class="comment">// namespace pyg</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这一部分用c++实现了index_sort的操作接口，只声明index_sort()的参数表。使用 <code>TORCH_LIBRARY_FRAGMENT</code> 注册操作的名称和签名（schema）。<code>TORCH_LIBRARY_FRAGMENT</code>和<code>TORCH_SELECTIVE_SCHEMA</code>均由pytorch提供。</p><h4 id="2-1-2-index-sort-kernal-cpp"><a href="#2-1-2-index-sort-kernal-cpp" class="headerlink" title="2.1.2 index_sort_kernal.cpp"></a>2.1.2 index_sort_kernal.cpp</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;tuple&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;ATen/ATen.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;ATen/Parallel.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/library.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;radix_sort.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> pyg &#123;</span><br><span class="line"><span class="keyword">namespace</span> ops &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="function">std::tuple&lt;at::Tensor, at::Tensor&gt; <span class="title">index_sort_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> at::Tensor&amp; input,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> at::optional&lt;<span class="type">int64_t</span>&gt; max)</span> </span>&#123;</span><br><span class="line"> <span class="comment">//具体实现</span></span><br><span class="line"></span><br><span class="line">&#125;  <span class="comment">// namespace</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">TORCH_LIBRARY_IMPL</span>(pyg, CPU, m) &#123;</span><br><span class="line">  m.<span class="built_in">impl</span>(<span class="built_in">TORCH_SELECTIVE_NAME</span>(<span class="string">&quot;pyg::index_sort&quot;</span>), <span class="built_in">TORCH_FN</span>(index_sort_kernel));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;  <span class="comment">// namespace ops</span></span><br><span class="line">&#125;  <span class="comment">// namespace pyg</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这里是<code>index_sort()</code>方法的核心实现，被命名为<code>index_sort_kernal()</code>。通过<code>TORCH_LIBRARY_IMPL</code>将<code>index_sort_kernel</code>绑定到<code>index_sort</code>接口，当上层调用<code>index_sort</code>时将自动使用<code>index_sort_kernal()</code>实现。</p><h3 id="2-2-softmax"><a href="#2-2-softmax" class="headerlink" title="2.2 softmax()"></a>2.2 softmax()</h3><p>pyg-lib对于<code>softmax()</code>函数也仅仅提供了cpu加速，因此不涉及cuda，但与<code>index_sort()</code>不同，其实现涉及<code>autograd</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dim = dim + src.dim() <span class="keyword">if</span> dim &lt; <span class="number">0</span> <span class="keyword">else</span> dim</span><br><span class="line"><span class="keyword">return</span> torch.ops.pyg.softmax_csr(src, ptr, dim)</span><br></pre></td></tr></table></figure><p>其顶层在对<code>dim</code>进行条件判断之后，便直接调用了底层的c++实现。</p><h4 id="2-2-1-softmax-h-softmax-cpp"><a href="#2-2-1-softmax-h-softmax-cpp" class="headerlink" title="2.2.1 softmax.h softmax.cpp"></a>2.2.1 softmax.h softmax.cpp</h4><p>这两个文件用于为<code>softmax()</code>绑定接口。由于<code>softmax_csr</code> 在计算时涉及到的操作是自定义的、需要梯度计算的操作，因此需要因此需要使用 <code>c10::Dispatcher</code> 来进行调度并确保操作的正确性。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Performs softmax operations for each group.</span></span><br><span class="line"><span class="function">PYG_API at::Tensor <span class="title">softmax_csr</span><span class="params">(<span class="type">const</span> at::Tensor&amp; src,</span></span></span><br><span class="line"><span class="params"><span class="function">                               <span class="type">const</span> at::Tensor&amp; ptr,</span></span></span><br><span class="line"><span class="params"><span class="function">                               <span class="type">const</span> <span class="type">int64_t</span> dim)</span> </span>&#123;</span><br><span class="line">  at::TensorArg src_arg&#123;src, <span class="string">&quot;src&quot;</span>, <span class="number">0</span>&#125;;</span><br><span class="line">  at::TensorArg ptr_arg&#123;ptr, <span class="string">&quot;ptr&quot;</span>, <span class="number">1</span>&#125;;</span><br><span class="line">  at::CheckedFrom c&#123;<span class="string">&quot;softmax_csr&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">  at::<span class="built_in">checkAllDefined</span>(c, &#123;src_arg, ptr_arg&#125;);</span><br><span class="line">  at::<span class="built_in">checkContiguous</span>(c, src_arg);</span><br><span class="line">  at::<span class="built_in">checkContiguous</span>(c, ptr_arg);</span><br><span class="line"></span><br><span class="line">  <span class="type">static</span> <span class="keyword">auto</span> op = c10::Dispatcher::<span class="built_in">singleton</span>()</span><br><span class="line">                       .<span class="built_in">findSchemaOrThrow</span>(<span class="string">&quot;pyg::softmax_csr&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">                       .<span class="built_in">typed</span>&lt;<span class="keyword">decltype</span>(softmax_csr)&gt;();</span><br><span class="line">  <span class="keyword">return</span> op.<span class="built_in">call</span>(src, ptr, dim);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Computes gradient for grouped softmax operation.</span></span><br><span class="line"><span class="function">PYG_API at::Tensor <span class="title">softmax_csr_backward</span><span class="params">(<span class="type">const</span> at::Tensor&amp; out,</span></span></span><br><span class="line"><span class="params"><span class="function">                                        <span class="type">const</span> at::Tensor&amp; out_grad,</span></span></span><br><span class="line"><span class="params"><span class="function">                                        <span class="type">const</span> at::Tensor&amp; ptr,</span></span></span><br><span class="line"><span class="params"><span class="function">                                        <span class="type">const</span> <span class="type">int64_t</span> dim)</span> </span>&#123;</span><br><span class="line"><span class="comment">//与上面类似的检查</span></span><br><span class="line">  <span class="keyword">return</span> op.<span class="built_in">call</span>(out, out_grad, ptr, dim);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里使用<code>PYG_API</code>来定义两个函数接口，该宏由pyg-lib定义，主要用于跨平台符号的导出和导入控制，特别是针对 Windows 和其他操作系统的 DLL 处理。接口的检查与调用逻辑如下：</p><p><strong>输入检查：</strong></p><ul><li><code>at::checkAllDefined</code> 确保输入张量都已定义。</li><li><code>at::checkContiguous</code> 检查输入是否是连续的内存布局（某些操作依赖此特性）。</li></ul><p><strong>操作查找：</strong></p><ul><li>通过 <code>c10::Dispatcher::singleton()</code> 获取 PyTorch 调度器实例。</li><li>使用 <code>findSchemaOrThrow</code> 查找名为 <code>&quot;pyg::softmax_csr&quot;</code> 的操作，并确保找到。</li><li><code>typed&lt;decltype(softmax_csr)&gt;()</code> 用于将操作类型显式声明为 <code>softmax_csr</code> 的签名。</li></ul><p><strong>调用操作：</strong></p><ul><li>使用 <code>op.call</code> 调用实际的操作实现。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TORCH_LIBRARY_FRAGMENT(pyg, m) &#123;</span><br><span class="line">  m.<span class="keyword">def</span>(TORCH_SELECTIVE_SCHEMA(</span><br><span class="line">      <span class="string">&quot;pyg::softmax_csr(Tensor src, Tensor ptr, int dim=0) -&gt; Tensor&quot;</span>));</span><br><span class="line">  m.<span class="keyword">def</span>(TORCH_SELECTIVE_SCHEMA(</span><br><span class="line">      <span class="string">&quot;pyg::softmax_csr_backward(Tensor out, Tensor out_grad, &quot;</span></span><br><span class="line">      <span class="string">&quot;Tensor ptr, int dim=0) -&gt; Tensor&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同样使用pytorch提供的宏进行接口绑定。</p><h4 id="2-2-2-cpu-softmax-kernal-cpp"><a href="#2-2-2-cpu-softmax-kernal-cpp" class="headerlink" title="2.2.2 cpu&#x2F;softmax_kernal.cpp"></a>2.2.2 cpu&#x2F;softmax_kernal.cpp</h4><p>这一部分实现了一个 CSR（Compressed Sparse Row）格式下的 Softmax 操作，并进行了前向和反向计算的实现。通过并行化的方式，提高了计算效率。<code>softmax_csr_forward_kernel</code> 调用<code>softmax_csr_forward_kernel_impl</code> 来实现前向计算。</p><p><code>softmax_csr_backward_kernel</code> 调用了 <code>softmax_csr_backward_kernel_impl</code> 来实现反向传播计算。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TORCH_LIBRARY_IMPL</span>(pyg, CPU, m) &#123;</span><br><span class="line">  m.<span class="built_in">impl</span>(<span class="built_in">TORCH_SELECTIVE_NAME</span>(<span class="string">&quot;pyg::softmax_csr&quot;</span>),</span><br><span class="line">         <span class="built_in">TORCH_FN</span>(softmax_csr_forward_kernel));</span><br><span class="line">  m.<span class="built_in">impl</span>(<span class="built_in">TORCH_SELECTIVE_NAME</span>(<span class="string">&quot;pyg::softmax_csr_backward&quot;</span>),</span><br><span class="line">         <span class="built_in">TORCH_FN</span>(softmax_csr_backward_kernel));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2-2-3-autograd-softmax-kernal-cpp"><a href="#2-2-3-autograd-softmax-kernal-cpp" class="headerlink" title="2.2.3 autograd&#x2F;softmax_kernal.cpp"></a>2.2.3 autograd&#x2F;softmax_kernal.cpp</h4><p>这一部分通过继承 <code>torch::autograd::Function</code> 实现了一个自定义的 SoftmaxCSR 操作，支持自动求导功能。在前向传播中，执行了 CSR 格式的 softmax 计算，在反向传播中，计算了输入张量的梯度。通过 <code>TORCH_LIBRARY_IMPL</code> 宏将其注册到 PyTorch 的 Autograd 系统中，确保在进行反向传播时能够正确计算梯度。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TORCH_LIBRARY_IMPL</span>(pyg, Autograd, m) &#123;</span><br><span class="line">  m.<span class="built_in">impl</span>(<span class="built_in">TORCH_SELECTIVE_NAME</span>(<span class="string">&quot;pyg::softmax_csr&quot;</span>),</span><br><span class="line">         <span class="built_in">TORCH_FN</span>(softmax_csr_autograd));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-matmul"><a href="#2-3-matmul" class="headerlink" title="2.3 matmul()"></a>2.3 matmul()</h3><p>该方法分为两个,分别是<code>grouped_matmul</code> 和<code>segment_matmul</code> 。</p><p><code>grouped_matmul</code> 操作的输入是两个矩阵列表（<code>inputs</code> 和 <code>others</code>），每个列表的元素数目相同，每个组中的矩阵进行对应的矩阵乘法。</p><p><code>segment_matmul</code> 操作的输入是一个矩阵 <code>inputs</code>，一个表示段范围的向量 <code>ptr</code>，以及一个按段组织的 3D 矩阵 <code>other</code>，每个段对应一个矩阵乘法。</p><p>在python顶层，其调用分别为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">outs = <span class="built_in">list</span>(GroupedMatmul.apply(<span class="built_in">tuple</span>(inputs + others)))</span><br><span class="line">out = torch.ops.pyg.segment_matmul(inputs, ptr, other)</span><br></pre></td></tr></table></figure><p>PyTorch 使用<strong>动态分派机制</strong>来根据当前的设备（CPU或CUDA）选择合适的实现。<code>inputs</code> 和 <code>other</code> 的 <code>.device</code> 属性决定操作所需的设备类型（CPU 或 CUDA）。</p><h4 id="2-3-1-matmul-cpp"><a href="#2-3-1-matmul-cpp" class="headerlink" title="2.3.1 matmul.cpp"></a>2.3.1 matmul.cpp</h4><p>接口绑定：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TORCH_LIBRARY_FRAGMENT(pyg, m) &#123;</span><br><span class="line">  m.<span class="keyword">def</span>(TORCH_SELECTIVE_SCHEMA(</span><br><span class="line">      <span class="string">&quot;pyg::grouped_matmul(Tensor[] input, Tensor[] other) -&gt; Tensor[]&quot;</span>));</span><br><span class="line">  m.<span class="keyword">def</span>(TORCH_SELECTIVE_SCHEMA(</span><br><span class="line">      <span class="string">&quot;pyg::segment_matmul(Tensor input, Tensor ptr, Tensor other) -&gt; Tensor&quot;</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="2-3-2-cpu-matmul-cpp以及autograd-matmul-cpp"><a href="#2-3-2-cpu-matmul-cpp以及autograd-matmul-cpp" class="headerlink" title="2.3.2 cpu&#x2F;matmul.cpp以及autograd&#x2F;matmul.cpp"></a>2.3.2 cpu&#x2F;matmul.cpp以及autograd&#x2F;matmul.cpp</h4><p>实现绑定如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TORCH_LIBRARY_IMPL</span>(pyg, CPU, m) &#123;</span><br><span class="line">  m.<span class="built_in">impl</span>(<span class="built_in">TORCH_SELECTIVE_NAME</span>(<span class="string">&quot;pyg::grouped_matmul&quot;</span>),</span><br><span class="line">         <span class="built_in">TORCH_FN</span>(grouped_matmul_kernel));</span><br><span class="line">  m.<span class="built_in">impl</span>(<span class="built_in">TORCH_SELECTIVE_NAME</span>(<span class="string">&quot;pyg::segment_matmul&quot;</span>),</span><br><span class="line">         <span class="built_in">TORCH_FN</span>(segment_matmul_kernel));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TORCH_LIBRARY_IMPL</span>(pyg, Autograd, m) &#123;</span><br><span class="line">  m.<span class="built_in">impl</span>(<span class="built_in">TORCH_SELECTIVE_NAME</span>(<span class="string">&quot;pyg::segment_matmul&quot;</span>),</span><br><span class="line">         <span class="built_in">TORCH_FN</span>(segment_matmul_autograd));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先只有分段矩阵乘法需要自动求导，如果运算在cpu上进行则直接调用c++实现的接口。</p><h4 id="2-3-3-cuda-matmul-cu"><a href="#2-3-3-cuda-matmul-cu" class="headerlink" title="2.3.3 cuda&#x2F;matmul.cu"></a>2.3.3 cuda&#x2F;matmul.cu</h4><p>实现绑定如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TORCH_LIBRARY_IMPL</span>(pyg, CUDA, m) &#123;</span><br><span class="line">  m.<span class="built_in">impl</span>(<span class="built_in">TORCH_SELECTIVE_NAME</span>(<span class="string">&quot;pyg::grouped_matmul&quot;</span>),</span><br><span class="line">         <span class="built_in">TORCH_FN</span>(grouped_matmul_kernel));</span><br><span class="line">  m.<span class="built_in">impl</span>(<span class="built_in">TORCH_SELECTIVE_NAME</span>(<span class="string">&quot;pyg::segment_matmul&quot;</span>),</span><br><span class="line">         <span class="built_in">TORCH_FN</span>(segment_matmul_kernel));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果运算在gpu上进行则直接调用cuda实现的接口。</p><h2 id="3-编译及打包"><a href="#3-编译及打包" class="headerlink" title="3. 编译及打包"></a>3. 编译及打包</h2><h3 id="3-1-主要文件"><a href="#3-1-主要文件" class="headerlink" title="3.1 主要文件"></a>3.1 主要文件</h3><h4 id="setup-py"><a href="#setup-py" class="headerlink" title="setup.py"></a><code>setup.py</code></h4><ul><li>提供了 Python 包的安装配置。</li><li>定义了依赖项、构建选项以及如何通过 CMake 构建本地扩展模块。</li><li>包含对 <strong>MKL</strong>（Math Kernel Library）、<strong>CUDA</strong> 和 <strong>Triton</strong> 等高性能计算工具的支持。</li></ul><h4 id="CMakeLists-txt"><a href="#CMakeLists-txt" class="headerlink" title="CMakeLists.txt"></a><code>CMakeLists.txt</code></h4><ul><li>定义了 CMake 构建流程。</li><li>支持多种可选功能，如测试、基准测试、CUDA 支持以及 Python 绑定。</li><li>配置了外部依赖（如 parallel-hashmap 和 METIS）。</li></ul><h3 id="3-2-setup-py"><a href="#3-2-setup-py" class="headerlink" title="3.2 setup.py()"></a>3.2 setup.py()</h3><p><strong>依赖定义和版本信息</strong></p><ul><li><code>__version__</code> 定义了项目版本号（当前为 0.4.0）。</li><li><code>install_requires</code> 动态添加了对 <code>MKL</code> 的依赖（如 <code>mkl-include</code> 和 <code>mkl-static</code>），前提是启用了 <code>USE_MKL_BLAS</code> 环境变量。</li><li>提供了额外依赖项，如：<ul><li><code>triton_requires</code>：为 Triton 提供支持。</li><li><code>test_requires</code>：测试相关依赖。</li><li><code>dev_requires</code>：开发阶段使用的依赖（如 <code>pre-commit</code>）。</li></ul></li></ul><p><strong>CMake 的集成</strong></p><ul><li><code>CMakeExtension</code>类：<ul><li>用于定义需要使用 CMake 构建的扩展模块（此处为 <code>libpyg</code>）。</li></ul></li><li><code>CMakeBuild</code>类：<ul><li>定制 <code>build_ext</code> 阶段，使用 CMake 生成构建系统并执行编译。</li><li>自动检测并设置 CUDA、MKL、Ninja 等选项，控制构建行为。</li><li>支持通过环境变量配置构建类型（<code>DEBUG</code>、<code>RELEASE</code> 等）。</li></ul></li></ul><p><strong>环境变量支持</strong></p><ul><li><code>USE_MKL_BLAS=1</code>：启用 MKL 支持（需要 PyTorch 自身也支持 MKL）。</li><li><code>FORCE_CUDA=1</code>：强制启用 CUDA 支持，无视自动检测结果。</li><li><code>DEBUG</code> 或 <code>REL_WITH_DEB_INFO</code>：控制编译模式。</li><li><code>FORCE_NINJA=1</code>：强制使用 Ninja 构建工具。</li></ul><p><strong>模块构建与安装</strong></p><ul><li>扩展模块 <code>libpyg</code> 使用 CMake 构建，最终结果以 Python 可加载的 <code>.so</code> 文件形式输出。</li></ul><p><strong>其他功能</strong></p><ul><li>提供警告信息，如缺少 Ninja 会影响构建速度。</li><li>动态解析 PyTorch 的配置，确定 MKL 的版本和支持状态</li></ul><h3 id="3-3-CmakeLists-txt"><a href="#3-3-CmakeLists-txt" class="headerlink" title="3.3 CmakeLists.txt"></a>3.3 CmakeLists.txt</h3><h4 id="3-3-1-基础设置"><a href="#3-3-1-基础设置" class="headerlink" title="3.3.1 基础设置"></a>3.3.1 基础设置</h4><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.15</span>)</span><br><span class="line"><span class="keyword">project</span>(pyg)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_STANDARD <span class="number">17</span>)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_STANDARD_REQUIRED <span class="keyword">ON</span>)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_SHARED_LIBRARY_PREFIX <span class="string">&quot;lib&quot;</span>)</span><br><span class="line"><span class="keyword">set</span>(PYG_VERSION <span class="number">0.4</span>.<span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong><code>cmake_minimum_required</code></strong>: 指定支持的最低 CMake 版本为 3.15。</p><p><strong><code>project</code></strong>: 定义工程名称为 <code>pyg</code>。</p><p><strong><code>set(CMAKE_CXX_STANDARD 17)</code></strong>: 使用 C++17 标准。</p><p><strong><code>set(CMAKE_SHARED_LIBRARY_PREFIX &quot;lib&quot;)</code></strong>: 动态库文件前缀设置为 <code>lib</code>（如 <code>libpyg.so</code>）。</p><p><strong><code>set(PYG_VERSION 0.4.0)</code></strong>: 定义项目的版本号。</p><h4 id="3-3-2-构建选项"><a href="#3-3-2-构建选项" class="headerlink" title="3.3.2 构建选项"></a>3.3.2 构建选项</h4><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">option</span>(BUILD_TEST <span class="string">&quot;Enable testing&quot;</span> <span class="keyword">OFF</span>)</span><br><span class="line"><span class="keyword">option</span>(BUILD_BENCHMARK <span class="string">&quot;Enable benchmarks&quot;</span> <span class="keyword">OFF</span>)</span><br><span class="line"><span class="keyword">option</span>(WITH_COV <span class="string">&quot;Enable code coverage&quot;</span> <span class="keyword">OFF</span>)</span><br><span class="line"><span class="keyword">option</span>(USE_PYTHON <span class="string">&quot;Link to Python when building&quot;</span> <span class="keyword">OFF</span>)</span><br><span class="line"><span class="keyword">option</span>(WITH_CUDA <span class="string">&quot;Enable CUDA support&quot;</span> <span class="keyword">OFF</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong><code>option</code></strong>: 定义可选配置项，后面是选项说明和默认值。</p><ul><li><code>BUILD_TEST</code>: 是否启用测试（默认关闭）。</li><li><code>BUILD_BENCHMARK</code>: 是否启用性能基准测试。</li><li><code>WITH_COV</code>: 是否开启代码覆盖率分析。</li><li><code>USE_PYTHON</code>: 是否支持 Python 绑定。</li><li><code>WITH_CUDA</code>: 是否启用 CUDA 支持。</li></ul><h4 id="3-3-3-ABI设置"><a href="#3-3-3-ABI设置" class="headerlink" title="3.3.3 ABI设置"></a>3.3.3 ABI设置</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -D_GLIBCXX_USE_CXX11_ABI=0&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>设置 <code>CXX11 ABI</code> 的兼容性选项，确保编译的二进制与依赖库（如 PyTorch）兼容。</p><h4 id="3-3-4-MKL设置"><a href="#3-3-4-MKL设置" class="headerlink" title="3.3.4 MKL设置"></a>3.3.4 MKL设置</h4><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>(WITH_MKL_BLAS <span class="number">0</span>)</span><br><span class="line"><span class="keyword">if</span>(USE_MKL_BLAS <span class="keyword">AND</span> <span class="keyword">DEFINED</span> BLAS_INCLUDE_DIR)</span><br><span class="line">  <span class="keyword">find_file</span>(MKL_INCLUDE_FOUND mkl.h <span class="variable">$&#123;BLAS_INCLUDE_DIR&#125;</span> NO_DEFAULT_PATH)</span><br><span class="line">  <span class="keyword">if</span>(MKL_INCLUDE_FOUND)</span><br><span class="line">    <span class="keyword">set</span>(WITH_MKL_BLAS <span class="number">1</span>)</span><br><span class="line">  <span class="keyword">else</span>()</span><br><span class="line">    <span class="keyword">if</span>(WITH_COV)</span><br><span class="line">      <span class="keyword">message</span>(FATAL_ERROR <span class="string">&quot;The mkl.h file was not found - pass the correct directory or set USE_MKL_BLAS=OFF.&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>()</span><br><span class="line">      <span class="keyword">message</span>(WARNING <span class="string">&quot;The mkl.h file was not found - building pyg-lib without MKL BLAS support.&quot;</span>)</span><br><span class="line">    <span class="keyword">endif</span>()</span><br><span class="line">  <span class="keyword">endif</span>()</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong><code>USE_MKL_BLAS</code></strong> 和 <code>BLAS_INCLUDE_DIR</code>:</p><ul><li>如果启用 MKL 支持，检查是否能找到 <code>mkl.h</code> 头文件。</li><li>找到后，启用 MKL 支持（<code>WITH_MKL_BLAS=1</code>）；否则发出警告或终止。</li></ul><h4 id="3-3-5-生成配置文件"><a href="#3-3-5-生成配置文件" class="headerlink" title="3.3.5 生成配置文件"></a>3.3.5 <strong>生成配置文件</strong></h4><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">configure_file</span>(<span class="variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/pyg_lib/csrc/config.h.in <span class="string">&quot;$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/pyg_lib/csrc/config.h&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>使用模板文件 <code>config.h.in</code> 生成 <code>config.h</code>，根据构建选项动态设置头文件内容。</li></ul><h4 id="3-3-6-Python-支持"><a href="#3-3-6-Python-支持" class="headerlink" title="3.3.6 Python 支持"></a>3.3.6 Python 支持</h4><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (USE_PYTHON)</span><br><span class="line">  <span class="keyword">cmake_policy</span>(<span class="keyword">SET</span> CMP0094 NEW)</span><br><span class="line">  <span class="keyword">set</span>(Python3_FIND_REGISTRY <span class="string">&quot;LAST&quot;</span>)</span><br><span class="line">  <span class="keyword">set</span>(Python3_FIND_FRAMEWORK <span class="string">&quot;LAST&quot;</span>)</span><br><span class="line">  <span class="keyword">set</span>(Python3_FIND_VIRTUALENV <span class="string">&quot;FIRST&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">add_definitions</span>(-DUSE_PYTHON)</span><br><span class="line">  <span class="keyword">find_package</span>(Python3 REQUIRED COMPONENTS Development)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><code>USE_PYTHON</code><ul><li>启用后，使用 <code>find_package</code> 查找 Python 开发工具链。</li><li>定义了如何优先查找虚拟环境中的 Python 和 Python 框架。</li><li>添加 <code>-DUSE_PYTHON</code> 宏，用于 C++ 源码中开启相关功能。</li></ul></li></ul><h4 id="3-3-7-CUDA-支持"><a href="#3-3-7-CUDA-支持" class="headerlink" title="3.3.7 CUDA 支持"></a>3.3.7 CUDA 支持</h4><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(WITH_CUDA)</span><br><span class="line">  <span class="keyword">enable_language</span>(CUDA)</span><br><span class="line">  <span class="keyword">add_definitions</span>(-DWITH_CUDA)</span><br><span class="line">  <span class="keyword">set</span>(CMAKE_CUDA_FLAGS <span class="string">&quot;$&#123;CMAKE_CUDA_FLAGS&#125; --expt-relaxed-constexpr&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">NOT</span> <span class="string">&quot;$ENV&#123;EXTERNAL_CUTLASS_INCLUDE_DIR&#125;&quot;</span> <span class="keyword">STREQUAL</span> <span class="string">&quot;&quot;</span>)</span><br><span class="line">    <span class="keyword">include_directories</span>($ENV&#123;EXTERNAL_CUTLASS_INCLUDE_DIR&#125;)</span><br><span class="line">  <span class="keyword">else</span>()</span><br><span class="line">    <span class="keyword">set</span>(CUTLASS_DIR third_party/cutlass/<span class="keyword">include</span>)</span><br><span class="line">    <span class="keyword">include_directories</span>(<span class="variable">$&#123;CUTLASS_DIR&#125;</span>)</span><br><span class="line">    <span class="keyword">set</span>(CUTLASS_UTIL_DIR third_party/cutlass/tools/util/<span class="keyword">include</span>)</span><br><span class="line">    <span class="keyword">include_directories</span>(<span class="variable">$&#123;CUTLASS_UTIL_DIR&#125;</span>)</span><br><span class="line">  <span class="keyword">endif</span>()</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong><code>enable_language(CUDA)</code></strong>: 启用 CUDA 编译器。</p><p><strong><code>add_definitions(-DWITH_CUDA)</code></strong>: 添加 CUDA 支持的预定义宏。</p><p><strong><code>CUTLASS</code></strong>: 设置 CUTLASS 库路径，这是一个高效的矩阵计算库，CUDA 内核依赖它。</p><h4 id="3-3-8-源码收集和目标构建"><a href="#3-3-8-源码收集和目标构建" class="headerlink" title="3.3.8 源码收集和目标构建"></a>3.3.8 源码收集和目标构建</h4><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>(CSRC pyg_lib/csrc)</span><br><span class="line"><span class="keyword">file</span>(GLOB_RECURSE ALL_SOURCES <span class="variable">$&#123;CSRC&#125;</span>/*.cpp)</span><br><span class="line"><span class="keyword">if</span> (WITH_CUDA)</span><br><span class="line">  <span class="keyword">file</span>(GLOB_RECURSE ALL_SOURCES <span class="variable">$&#123;ALL_SOURCES&#125;</span> <span class="variable">$&#123;CSRC&#125;</span>/*.cu)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"><span class="keyword">file</span>(GLOB_RECURSE ALL_HEADERS <span class="variable">$&#123;CSRC&#125;</span>/*.h)</span><br><span class="line"><span class="keyword">add_library</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> SHARED <span class="variable">$&#123;ALL_SOURCES&#125;</span>)</span><br><span class="line"><span class="keyword">target_include_directories</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> PUBLIC <span class="string">&quot;$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;&quot;</span>)</span><br><span class="line"><span class="keyword">if</span>(MKL_INCLUDE_FOUND)</span><br><span class="line">    <span class="keyword">target_include_directories</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> PRIVATE <span class="variable">$&#123;BLAS_INCLUDE_DIR&#125;</span>)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong><code>file(GLOB_RECURSE)</code></strong>: 递归收集源码文件，包括 <code>.cpp</code> 和 <code>.cu</code> 文件。</p><p><strong><code>add_library</code></strong>: 创建动态库目标 <code>pyg</code>，并关联所有源码文件。</p><p><strong><code>target_include_directories</code></strong>:</p><ul><li>添加当前目录为头文件路径。</li><li>如果找到 MKL，添加其头文件路径。</li></ul><h4 id="3-3-9-外部依赖"><a href="#3-3-9-外部依赖" class="headerlink" title="3.3.9 外部依赖"></a>3.3.9 外部依赖</h4><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (<span class="keyword">NOT</span> <span class="string">&quot;$ENV&#123;EXTERNAL_PHMAP_INCLUDE_DIR&#125;&quot;</span> <span class="keyword">STREQUAL</span> <span class="string">&quot;&quot;</span>)</span><br><span class="line">  <span class="keyword">include_directories</span>($ENV&#123;EXTERNAL_PHMAP_INCLUDE_DIR&#125;)</span><br><span class="line"><span class="keyword">else</span>()</span><br><span class="line">  <span class="keyword">set</span>(PHMAP_DIR third_party/parallel-hashmap)</span><br><span class="line">  <span class="keyword">target_include_directories</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> PRIVATE <span class="variable">$&#123;PHMAP_DIR&#125;</span>)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong><code>parallel-hashmap</code></strong>: 检查 <code>parallel-hashmap</code> 的路径，优先使用环境变量提供的路径，否则使用 <code>third_party</code> 中的副本。</p><h4 id="3-3-10-平台特定设置"><a href="#3-3-10-平台特定设置" class="headerlink" title="3.3.10 平台特定设置"></a>3.3.10 平台特定设置</h4><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (MSVC)</span><br><span class="line">  <span class="keyword">set</span>(CMAKE_C_FLAGS <span class="string">&quot;$&#123;CMAKE_C_FLAGS&#125; /DIDXTYPEWIDTH=64 /DREALTYPEWIDTH=32&quot;</span>)</span><br><span class="line">  <span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; /DIDXTYPEWIDTH=64 /DREALTYPEWIDTH=32&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>()</span><br><span class="line">  <span class="keyword">set</span>(CMAKE_C_FLAGS <span class="string">&quot;$&#123;CMAKE_C_FLAGS&#125; -DIDXTYPEWIDTH=64 -DREALTYPEWIDTH=32&quot;</span>)</span><br><span class="line">  <span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -DIDXTYPEWIDTH=64 -DREALTYPEWIDTH=32&quot;</span>)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>设置索引宽度和实数类型宽度，支持 64 位索引和 32 位浮点运算。</p><h4 id="3-3-11-METIS-集成"><a href="#3-3-11-METIS-集成" class="headerlink" title="3.3.11 METIS 集成"></a>3.3.11 METIS 集成</h4><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (<span class="keyword">NOT</span> MSVC)</span><br><span class="line">  <span class="keyword">set</span>(METIS_DIR third_party/METIS)</span><br><span class="line">  <span class="keyword">target_include_directories</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> PRIVATE <span class="variable">$&#123;METIS_DIR&#125;</span>/<span class="keyword">include</span>)</span><br><span class="line">  <span class="keyword">set</span>(GKLIB_PATH <span class="string">&quot;$&#123;METIS_DIR&#125;/GKlib&quot;</span>)</span><br><span class="line">  <span class="keyword">include</span>(<span class="variable">$&#123;GKLIB_PATH&#125;</span>/GKlibSystem.cmake)</span><br><span class="line">  <span class="keyword">include_directories</span>(<span class="variable">$&#123;GKLIB_PATH&#125;</span>)</span><br><span class="line">  <span class="keyword">include_directories</span>(<span class="string">&quot;$&#123;METIS_DIR&#125;/include&quot;</span>)</span><br><span class="line">  <span class="keyword">add_subdirectory</span>(<span class="string">&quot;$&#123;METIS_DIR&#125;/libmetis&quot;</span>)</span><br><span class="line">  <span class="keyword">target_link_libraries</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> PRIVATE metis)</span><br><span class="line"><span class="keyword">endif</span>()</span><br></pre></td></tr></table></figure><p>metis外部工具集成</p><h4 id="3-3-12-PyTorch-和-OpenMP"><a href="#3-3-12-PyTorch-和-OpenMP" class="headerlink" title="3.3.12 PyTorch 和 OpenMP"></a>3.3.12 PyTorch 和 OpenMP</h4><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">find_package</span>(Torch REQUIRED)</span><br><span class="line"><span class="keyword">message</span>(<span class="string">&quot;-- TORCH_LIBRARIES: $&#123;TORCH_LIBRARIES&#125;&quot;</span>)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> PRIVATE <span class="variable">$&#123;TORCH_LIBRARIES&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">find_package</span>(OpenMP)</span><br><span class="line"><span class="keyword">if</span>(OpenMP_CXX_FOUND)</span><br><span class="line">  <span class="keyword">target_link_libraries</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> PRIVATE OpenMP::OpenMP_CXX)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查找并链接 PyTorch 和 OpenMP 库。</p>]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pyg-lib代码分析</title>
    <link href="/2024/12/03/pyg-lib%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/"/>
    <url>/2024/12/03/pyg-lib%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h2 id="1-PyG-lib"><a href="#1-PyG-lib" class="headerlink" title="1. PyG-lib"></a>1. PyG-lib</h2><p><code>pyg_lib</code> 是一个为 PyG（PyTorch Geometric）和 PyTorch 设计的底层图神经网络库。 </p><p>该库提供以下主要功能：</p><ul><li><strong><code>cuda_version()</code></strong>：返回编译 <code>pyg_lib</code> 时所使用的 CUDA 版本。 </li><li><strong><code>get_home_dir()</code></strong>：获取用于存储所有 <code>pyg-lib</code> 数据的缓存目录。如果未调用 <code>set_home_dir()</code>，则路径由环境变量 <code>$PYG_LIB_HOME</code> 指定，默认为 <code>~/.cache/pyg_lib</code>。 </li><li><strong><code>set_home_dir(path)</code></strong>：设置用于存储所有 <code>pyg-lib</code> 数据的缓存目录。参数 <code>path</code> 是本地文件夹的路径。</li></ul><p>此外，<code>pyg_lib</code> 还包含以下子模块：</p><ul><li><strong><code>pyg_lib.ops</code></strong>：提供操作函数。 </li><li><strong><code>pyg_lib.sampler</code></strong>：提供采样器功能。 </li><li><strong><code>pyg_lib.partition</code></strong>：提供图分区功能。</li></ul><p>这些功能和模块为开发者提供了构建和操作图神经网络的基础工具。文档主要对pyg-lib的benchmark以及test目录下的demo进行代码分析。</p><h2 id="2-ops"><a href="#2-ops" class="headerlink" title="2. ops"></a>2. ops</h2><h3 id="2-1-index-sort"><a href="#2-1-index-sort" class="headerlink" title="2.1 index_sort()"></a>2.1 index_sort()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index_sort(inputs: Tensor, max_value: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>) → <span class="type">Tuple</span>[Tensor, Tensor]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="功能描述"><a href="#功能描述" class="headerlink" title="功能描述"></a><strong>功能描述</strong></h4><p>将 <code>inputs</code> 张量的元素按升序排序。要求 <code>inputs</code> 是一维的，并且只包含正整数值。如果提供了 <code>max_value</code>，底层算法可以使用该值来提高性能。</p><h4 id="输入参数"><a href="#输入参数" class="headerlink" title="输入参数"></a><strong>输入参数</strong></h4><ul><li><code>inputs</code>：一个一维张量（<code>Tensor</code>），其元素应为正整数。</li><li><code>max_value</code>：一个可选的整数（<code>Optional[int]</code>），表示输入张量中可能的最大值。默认值为 <code>None</code>。</li></ul><h4 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a><strong>返回值</strong></h4><ul><li>返回一个元组，包含两个张量：<ol><li><strong>排序后的张量</strong>（<code>Tensor</code>），即按升序排序后的 <code>inputs</code>。</li><li><strong>排序索引</strong>（<code>Tensor</code>），表示每个元素在排序前的位置。</li></ol></li></ul><h4 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a><strong>具体实现</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">index_sort</span>(<span class="params"></span></span><br><span class="line"><span class="params">    inputs: Tensor,</span></span><br><span class="line"><span class="params">    max_value: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[Tensor, Tensor]:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> inputs.is_cpu:</span><br><span class="line">        <span class="keyword">return</span> torch.sort(inputs)</span><br><span class="line">    <span class="keyword">return</span> torch.ops.pyg.index_sort(inputs, max_value)</span><br></pre></td></tr></table></figure><p>根据 <code>inputs</code> 所在的设备（CPU 或 GPU），选择不同的排序实现。如果 <code>inputs</code> 存在于 GPU 上，直接使用 PyTorch 的内置排序；如果 <code>inputs</code> 在 CPU 上，则使用 PyG 自定义的排序操作，可能是为 CPU 优化的版本。这一部分具体用c++实现。</p><h3 id="2-2-segment-matmul"><a href="#2-2-segment-matmul" class="headerlink" title="2.2 segment_matmul()"></a>2.2 segment_matmul()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">segment_matmul</span>(<span class="params"></span></span><br><span class="line"><span class="params">    inputs: Tensor,</span></span><br><span class="line"><span class="params">    ptr: Tensor,</span></span><br><span class="line"><span class="params">    other: Tensor,</span></span><br><span class="line"><span class="params">    bias: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; Tensor:</span><br><span class="line">   </span><br></pre></td></tr></table></figure><h4 id="功能描述-1"><a href="#功能描述-1" class="headerlink" title="功能描述"></a><strong>功能描述</strong></h4><p><code>segment_matmul</code> 函数实现了一个按段执行的矩阵乘法操作。它将输入的二维矩阵 <code>inputs</code> 按照 <code>ptr</code> 指定的分段信息划分成多个段，并与三维矩阵 <code>other</code> 中的每个分段进行矩阵乘法，最终将所有分段的结果拼接起来，返回一个二维输出矩阵。这个操作适用于需要按分组（或段）进行矩阵计算的情况。</p><p>函数的特点是：</p><ul><li>它通过一个优化的内核来实现分段矩阵乘法，能有效并行计算多个分段。</li><li>支持可选的偏置项 <code>bias</code>，在计算每个分段的矩阵乘法结果时加上偏置。</li></ul><h4 id="输入参数-1"><a href="#输入参数-1" class="headerlink" title="输入参数"></a><strong>输入参数</strong></h4><p><strong><code>inputs</code></strong> (Tensor)：一个二维张量，形状为 <code>[N, K]</code>，表示左操作数。<code>N</code> 是样本数量，<code>K</code> 是每个样本的特征维度。</p><p><strong><code>ptr</code></strong> (Tensor)：一个一维张量，形状为 <code>[B + 1]</code>，表示分段的边界。<code>ptr[i]</code> 和 <code>ptr[i + 1]</code> 之间的元素属于第 <code>i</code> 个分段。<code>B</code> 是分段的数量。</p><p><strong><code>other</code></strong> (Tensor)：一个三维张量，形状为 <code>[B, K, M]</code>，表示右操作数。<code>B</code> 个分段中的每个分段是一个 <code>K x M</code> 的矩阵，用来与 <code>inputs</code> 的每个分段进行矩阵乘法。</p><p><strong><code>bias</code></strong> (Optional[Tensor])：一个可选的二维张量，形状为 <code>[B, M]</code>，表示每个分段的偏置项。如果不为 <code>None</code>，则在每个分段的结果中加上相应的偏置。</p><h4 id="返回值-1"><a href="#返回值-1" class="headerlink" title="返回值"></a><strong>返回值</strong></h4><p>返回值是一个二维张量 <code>out</code>，形状为 <code>[N, M]</code>，表示矩阵乘法的最终结果。每个分段的矩阵乘法结果会依次合并在一起，形成一个完整的二维矩阵。</p><p>如果 <code>bias</code> 参数不为 <code>None</code>，则返回的 <code>out</code> 将在每个分段的结果上加上对应的偏置。</p><h4 id="具体实现-1"><a href="#具体实现-1" class="headerlink" title="具体实现"></a><strong>具体实现</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">out = torch.ops.pyg.segment_matmul(inputs, ptr, other)</span><br><span class="line"><span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(ptr.numel() - <span class="number">1</span>):</span><br><span class="line">        out[ptr[i]:ptr[i + <span class="number">1</span>]] += bias[i]</span><br><span class="line"><span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>这里调用了 pyg-lib 提供的优化版本的 <code>segment_matmul</code> 操作。该操作会按 <code>ptr</code> 指定的分段边界，将 <code>inputs</code> 中的数据分成多个段，并与 <code>other</code> 中的对应分段进行矩阵乘法。最终将所有分段的计算结果拼接成一个完整的二维矩阵 <code>out</code>。如果提供了偏置 <code>bias</code>，则对每个分段的结果加上对应的偏置项。<code>ptr</code> 张量指定了每个分段的边界，<code>out[ptr[i]:ptr[i + 1]]</code> 获取对应分段的计算结果，并将偏置加到该分段中。</p><h3 id="2-3-grouped-matmul"><a href="#2-3-grouped-matmul" class="headerlink" title="2.3 grouped_matmul()"></a>2.3 grouped_matmul()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">grouped_matmul</span>(<span class="params"></span></span><br><span class="line"><span class="params">    inputs: <span class="type">List</span>[Tensor],</span></span><br><span class="line"><span class="params">    others: <span class="type">List</span>[Tensor],</span></span><br><span class="line"><span class="params">    biases: <span class="type">Optional</span>[<span class="type">List</span>[Tensor]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">List</span>[Tensor]:</span><br></pre></td></tr></table></figure><h4 id="功能描述-2"><a href="#功能描述-2" class="headerlink" title="功能描述"></a><strong>功能描述</strong></h4><p>该函数执行的是一种按组分配的矩阵乘法，它对多个左操作数（<code>inputs</code>）和右操作数（<code>others</code>）分别进行矩阵乘法，使用并行计算来提高性能。每个组对应一对输入矩阵和输出矩阵，函数在计算时按组来处理。</p><ul><li><strong>输入矩阵</strong>：每一组的左操作数（<code>inputs</code>）都是一个二维矩阵（形状为 <code>[N_i, K_i]</code>），右操作数（<code>others</code>）也是一个二维矩阵（形状为 <code>[K_i, M_i]</code>）。矩阵的维度是动态的，每一组的矩阵维度可以不同。</li><li><strong>计算过程</strong>：每一组进行矩阵乘法操作，得到一个输出矩阵。</li><li><strong>并行计算</strong>：该实现通过专门的内核函数并行化处理每一组的矩阵乘法，能够提高性能。</li></ul><h4 id="输入参数-2"><a href="#输入参数-2" class="headerlink" title="输入参数"></a><strong>输入参数</strong></h4><p><strong>inputs</strong>（类型：<code>List[Tensor]</code>）：包含多个二维矩阵的列表，表示左操作数，每个矩阵的形状为 <code>[N_i, K_i]</code>，其中 <code>N_i</code> 是行数，<code>K_i</code> 是列数。</p><p><strong>others</strong>（类型：<code>List[Tensor]</code>）：包含多个二维矩阵的列表，表示右操作数，每个矩阵的形状为 <code>[K_i, M_i]</code>，其中 <code>K_i</code> 和左操作数中的 <code>K_i</code> 必须匹配。</p><p><strong>biases</strong>（类型：<code>Optional[List[Tensor]]</code>，默认为 <code>None</code>）：每个输出矩阵的可选偏置项，偏置的形状为 <code>[M_i]</code>，即每个输出矩阵有一个偏置，形状和每个输出矩阵的列数相同。如果提供了偏置，结果矩阵会加上相应的偏置。</p><h4 id="返回值-2"><a href="#返回值-2" class="headerlink" title="返回值"></a><strong>返回值</strong></h4><p>返回一个包含多个二维矩阵的列表。每个矩阵的形状为 <code>[N_i, M_i]</code>，即每个输出矩阵是输入矩阵和相应的右操作数矩阵相乘得到的结果。</p><h4 id="具体实现-2"><a href="#具体实现-2" class="headerlink" title="具体实现"></a><strong>具体实现</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">outs = <span class="built_in">list</span>(GroupedMatmul.apply(<span class="built_in">tuple</span>(inputs + others)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> biases <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(biases)):</span><br><span class="line">        outs[i] = outs[i] + biases[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outs</span><br></pre></td></tr></table></figure><p><code>GroupedMatmul.apply</code> 是一个自定义的运算，它执行了输入矩阵和右操作矩阵的批量矩阵乘法。我们将在下面介绍<code>inputs + others</code> 是将 <code>inputs</code> 和 <code>others</code> 两个列表合并为一个元组，传递给 <code>GroupedMatmul.apply</code> 进行矩阵乘法计算。如果提供了偏置项 <code>biases</code>，则将每个输出矩阵的相应偏置加到对应的输出矩阵中。对于每一组，输出矩阵会加上相应的偏置。</p><h4 id="GroupedMatmul介绍"><a href="#GroupedMatmul介绍" class="headerlink" title="GroupedMatmul介绍"></a><strong>GroupedMatmul介绍</strong></h4><p><code>GroupedMatmul</code> 是一个自定义的 PyTorch 运算（通过 <code>torch.autograd.Function</code> 类定义），它执行的是批量矩阵乘法，并能够自动计算矩阵乘法过程中的梯度。它特别适用于在每个组中有多个矩阵对的场景，通过高效的并行化计算提升性能。</p><ul><li><p>forward()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, args: <span class="type">Tuple</span>[Tensor]</span>) -&gt; <span class="type">Tuple</span>[Tensor]:</span><br><span class="line">       ctx.save_for_backward(*args)</span><br><span class="line">  </span><br><span class="line">       inputs: <span class="type">List</span>[Tensor] = [x <span class="keyword">for</span> x <span class="keyword">in</span> args[:<span class="built_in">int</span>(<span class="built_in">len</span>(args) / <span class="number">2</span>)]]</span><br><span class="line">       others: <span class="type">List</span>[Tensor] = [other <span class="keyword">for</span> other <span class="keyword">in</span> args[<span class="built_in">int</span>(<span class="built_in">len</span>(args) / <span class="number">2</span>):]]</span><br><span class="line">       outs = torch.ops.pyg.grouped_matmul(inputs, others)</span><br><span class="line">  </span><br><span class="line">       <span class="comment"># NOTE Autograd doesnt set `out[i].requires_grad = True` automatically</span></span><br><span class="line">       <span class="keyword">for</span> x, other, out <span class="keyword">in</span> <span class="built_in">zip</span>(inputs, others, outs):</span><br><span class="line">           <span class="keyword">if</span> x.requires_grad <span class="keyword">or</span> other.requires_grad:</span><br><span class="line">               out.requires_grad = <span class="literal">True</span></span><br><span class="line">  </span><br><span class="line">       <span class="keyword">return</span> <span class="built_in">tuple</span>(outs)</span><br></pre></td></tr></table></figure><p><strong>保存输入张量</strong>：<code>ctx.save_for_backward(*args)</code> 保存输入的张量，以便在反向传播时使用。</p><p><strong>分割输入张量</strong>：将输入的元组 <code>args</code> 切分为 <code>inputs</code> 和 <code>others</code>。<code>inputs</code> 是左操作数的张量，<code>others</code> 是右操作数的张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs: <span class="type">List</span>[Tensor] = [x <span class="keyword">for</span> x <span class="keyword">in</span> args[:<span class="built_in">int</span>(<span class="built_in">len</span>(args) / <span class="number">2</span>)]]</span><br><span class="line">others: <span class="type">List</span>[Tensor] = [other <span class="keyword">for</span> other <span class="keyword">in</span> args[<span class="built_in">int</span>(<span class="built_in">len</span>(args) / <span class="number">2</span>):]]</span><br></pre></td></tr></table></figure><p><strong>执行矩阵乘法</strong>：通过 <code>torch.ops.pyg.grouped_matmul(inputs, others)</code> 调用pyg-lib中实现的矩阵乘法操作，并将结果存储在 <code>outs</code> 中。</p><p><strong>设置梯度</strong>：如果 <code>inputs</code> 或 <code>others</code> 中有一个张量的 <code>requires_grad</code> 为 <code>True</code>，则将对应的输出张量的 <code>requires_grad</code> 设置为 <code>True</code>，确保可以在反向传播时计算该输出的梯度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x, other, out <span class="keyword">in</span> <span class="built_in">zip</span>(inputs, others, outs):</span><br><span class="line">    <span class="keyword">if</span> x.requires_grad <span class="keyword">or</span> other.requires_grad:</span><br><span class="line">        out.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure></li><li><p>backward()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, *outs_grad: <span class="type">Tuple</span>[Tensor]</span>) -&gt; <span class="type">Tuple</span>[Tensor]:</span><br><span class="line">      args = ctx.saved_tensors</span><br><span class="line">      inputs: <span class="type">List</span>[Tensor] = [x <span class="keyword">for</span> x <span class="keyword">in</span> args[:<span class="built_in">int</span>(<span class="built_in">len</span>(outs_grad))]]</span><br><span class="line">      others: <span class="type">List</span>[Tensor] = [other <span class="keyword">for</span> other <span class="keyword">in</span> args[<span class="built_in">int</span>(<span class="built_in">len</span>(outs_grad)):]]</span><br><span class="line">  </span><br><span class="line">      inputs_grad = []</span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">any</span>([x.requires_grad <span class="keyword">for</span> x <span class="keyword">in</span> inputs]):</span><br><span class="line">          others = [other.t() <span class="keyword">for</span> other <span class="keyword">in</span> others]</span><br><span class="line">          inputs_grad = torch.ops.pyg.grouped_matmul(outs_grad, others)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          inputs_grad = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(outs_grad))]</span><br><span class="line">  </span><br><span class="line">      others_grad = []</span><br><span class="line">      <span class="keyword">if</span> <span class="built_in">any</span>([other.requires_grad <span class="keyword">for</span> other <span class="keyword">in</span> others]):</span><br><span class="line">          inputs = [x.t() <span class="keyword">for</span> x <span class="keyword">in</span> inputs]</span><br><span class="line">          others_grad = torch.ops.pyg.grouped_matmul(inputs, outs_grad)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          others_grad = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(outs_grad))]</span><br><span class="line">  </span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">tuple</span>(inputs_grad + others_grad)</span><br></pre></td></tr></table></figure><p><strong>获取保存的输入张量</strong>：通过 <code>ctx.saved_tensors</code> 获取保存的输入张量（<code>inputs</code> 和 <code>others</code>）。</p><p><strong>计算 <code>inputs</code> 的梯度</strong>：</p><ul><li>如果 <code>inputs</code> 中的任何张量要求梯度（即 <code>requires_grad</code> 为 <code>True</code>），则转置 <code>others</code> 并执行矩阵乘法。</li><li>使用 <code>torch.ops.pyg.grouped_matmul</code> 来计算反向传播时 <code>inputs</code> 的梯度。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">any</span>([x.requires_grad <span class="keyword">for</span> x <span class="keyword">in</span> inputs]):</span><br><span class="line">    others = [other.t() <span class="keyword">for</span> other <span class="keyword">in</span> others]</span><br><span class="line">    inputs_grad = torch.ops.pyg.grouped_matmul(outs_grad, others)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    inputs_grad = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(outs_grad))]</span><br></pre></td></tr></table></figure><p><strong>计算 <code>others</code> 的梯度</strong>：</p><ul><li>如果 <code>others</code> 中的任何张量要求梯度（即 <code>requires_grad</code> 为 <code>True</code>），则转置 <code>inputs</code> 并执行矩阵乘法。</li><li>使用 <code>torch.ops.pyg.grouped_matmul</code> 来计算反向传播时 <code>others</code> 的梯度。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">any</span>([other.requires_grad <span class="keyword">for</span> other <span class="keyword">in</span> others]):</span><br><span class="line">    inputs = [x.t() <span class="keyword">for</span> x <span class="keyword">in</span> inputs]</span><br><span class="line">    others_grad = torch.ops.pyg.grouped_matmul(inputs, outs_grad)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    others_grad = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(outs_grad))]</span><br></pre></td></tr></table></figure><p><strong>返回梯度</strong>：返回 <code>inputs_grad</code> 和 <code>others_grad</code>，这些是输入张量和右操作数矩阵在反向传播中的梯度。</p></li></ul><h3 id="2-4-fused-scatter-reduce"><a href="#2-4-fused-scatter-reduce" class="headerlink" title="2.4 fused_scatter_reduce()"></a>2.4 fused_scatter_reduce()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fused_scatter_reduce</span>(<span class="params"></span></span><br><span class="line"><span class="params">    inputs: Tensor,</span></span><br><span class="line"><span class="params">    index: Tensor,</span></span><br><span class="line"><span class="params">    dim_size: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    reduce_list: <span class="type">List</span>[<span class="built_in">str</span>],</span></span><br><span class="line"><span class="params"></span>) -&gt; Tensor:</span><br></pre></td></tr></table></figure><h4 id="功能描述-3"><a href="#功能描述-3" class="headerlink" title="功能描述"></a><strong>功能描述</strong></h4><p><code>fused_scatter_reduce</code> 函数的目标是将多个散点聚合操作合并成一个内核函数，减少多个内核函数调用的开销，提高执行效率。它支持以下聚合操作：</p><ul><li><strong>sum</strong>（求和）</li><li><strong>mean</strong>（均值）</li><li><strong>max</strong>（最大值）</li><li><strong>min</strong>（最小值）</li></ul><h4 id="输入参数-3"><a href="#输入参数-3" class="headerlink" title="输入参数"></a><strong>输入参数</strong></h4><p><strong>inputs</strong> (<code>Tensor</code>): 输入的张量，形状为 <code>[N, F]</code>，其中 <code>N</code> 是元素的数量，<code>F</code> 是每个元素的特征数。此张量包含了需要进行聚合的数值。</p><p><strong>index</strong> (<code>Tensor</code>): 形状为 <code>[N]</code> 的索引张量，用于指定每个元素所属的组。<code>inputs[i]</code> 被聚合到 <code>index[i]</code> 指定的组中。</p><p><strong>dim_size</strong> (<code>int</code>): 聚合后的目标维度大小（即结果张量的行数）。</p><p><strong>reduce_list</strong> (<code>List[str]</code>): 聚合操作的列表，支持的操作包括 <code>sum</code>、<code>mean</code>、<code>max</code> 和 <code>min</code>。</p><h4 id="返回值-3"><a href="#返回值-3" class="headerlink" title="返回值"></a><strong>返回值</strong></h4><p><strong>out</strong> (<code>Tensor</code>): 形状为 <code>[dim_size, len(reduce_list) * num_feats]</code> 的输出张量，包含了按给定聚合操作（<code>reduce_list</code>）聚合后的结果。每个组的多个聚合操作结果在该维度上按顺序排列。</p><h4 id="具体实现-3"><a href="#具体实现-3" class="headerlink" title="具体实现"></a><strong>具体实现</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span> inputs.is_floating_point()</span><br><span class="line"><span class="keyword">assert</span> inputs.is_cuda <span class="keyword">and</span> index.is_cuda</span><br><span class="line"><span class="keyword">assert</span> inputs.dim() == <span class="number">2</span> <span class="keyword">and</span> index.dim() == <span class="number">1</span></span><br><span class="line"><span class="keyword">assert</span> inputs.size(<span class="number">0</span>) == index.size(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> inputs.is_contiguous() <span class="keyword">and</span> index.is_contiguous()</span><br><span class="line">   num_feats = inputs.size(<span class="number">1</span>)</span><br><span class="line">   num_reductions = <span class="built_in">len</span>(reduce_list)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">assert</span> <span class="built_in">isinstance</span>(reduce_list, <span class="built_in">list</span>) <span class="keyword">and</span> <span class="built_in">len</span>(reduce_list) &lt;= NUM_REDUCTIONS</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> <span class="built_in">len</span>(reduce_list) &lt;= <span class="number">1</span>:</span><br><span class="line">       warnings.warn(<span class="string">f&quot;It is not recommended to call `fused_scatter_reduce` &quot;</span></span><br><span class="line">                     <span class="string">f&quot;with a single reduction (got <span class="subst">&#123;reduce_list&#125;</span>). Please &quot;</span></span><br><span class="line">                     <span class="string">f&quot;consider using vanilla `scatter_reduce_` instead.&quot;</span>)</span><br><span class="line"></span><br><span class="line">       reduce_slice_dict = &#123;</span><br><span class="line">           reduce: <span class="built_in">slice</span>(i * num_feats, (i + <span class="number">1</span>) * num_feats)</span><br><span class="line">           <span class="keyword">for</span> i, reduce <span class="keyword">in</span> <span class="built_in">enumerate</span>(reduce_list)</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">assert</span> <span class="built_in">len</span>(reduce_list) == <span class="built_in">len</span>(reduce_slice_dict)</span><br></pre></td></tr></table></figure><p>首先对输入进行验证，确保 <code>inputs</code> 和 <code>index</code> 是 CUDA 张量，且 <code>inputs</code> 是浮点数类型。确保 <code>inputs</code> 是一个二维张量，而 <code>index</code> 是一维张量，并且它们的第一维大小相同。对 <code>reduce_list</code> 进行检查，确保其长度不超过 <code>NUM_REDUCTIONS</code>（最大支持的聚合操作数目）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">out = inputs.new(dim_size, <span class="built_in">len</span>(reduce_list) * num_feats)</span><br><span class="line">   </span><br><span class="line">     <span class="comment"># Pre-processing: Take care of correct initialization for each reduction:</span></span><br><span class="line">   <span class="keyword">for</span> i, reduce <span class="keyword">in</span> <span class="built_in">enumerate</span>(reduce_list):</span><br><span class="line">       <span class="keyword">assert</span> reduce <span class="keyword">in</span> REDUCTIONS</span><br><span class="line">       <span class="keyword">if</span> reduce == <span class="string">&#x27;min&#x27;</span>:</span><br><span class="line">           fill_value = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">       <span class="keyword">elif</span> reduce == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">           fill_value = <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)</span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           fill_value = <span class="number">0.0</span></span><br><span class="line">       out[:, reduce_slice_dict[reduce]] = fill_value</span><br></pre></td></tr></table></figure><p>接下来，初始化一个大小为 <code>[dim_size, len(reduce_list) * num_feats]</code> 的输出张量 <code>out</code>，并根据不同的聚合操作（<code>reduce_list</code>）对其进行初始化：</p><ul><li>对于 <code>min</code> 聚合操作，初始化为 <code>float(&#39;inf&#39;)</code>。</li><li>对于 <code>max</code> 聚合操作，初始化为 <code>float(&#39;-inf&#39;)</code>。</li><li>对于 <code>sum</code> 和 <code>mean</code> 聚合操作，初始化为 0。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fill `reduce_list` with dummy values.</span></span><br><span class="line">  reduce_list = reduce_list + [NONE] * (NUM_REDUCTIONS - <span class="built_in">len</span>(reduce_list))</span><br></pre></td></tr></table></figure><p>如果 <code>reduce_list</code> 中的聚合操作少于 <code>NUM_REDUCTIONS</code>（最大支持的聚合操作数），则用 <code>NONE</code> 填充，确保后续的 GPU 内核调用能够正确处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">grid = <span class="keyword">lambda</span> meta: (  <span class="comment"># noqa: E731</span></span><br><span class="line">      triton.cdiv(inputs.numel(), meta[<span class="string">&#x27;BLOCK_SIZE&#x27;</span>]), )</span><br><span class="line"> _fused_scatter_reduce_forward_kernel[grid](</span><br><span class="line">      inputs,</span><br><span class="line">      index,</span><br><span class="line">      out,</span><br><span class="line">      num_feats,</span><br><span class="line">      num_reductions,</span><br><span class="line">      inputs.numel(),</span><br><span class="line">      OPT_REDUCTIONS.index(reduce_list[<span class="number">0</span>]),</span><br><span class="line">      OPT_REDUCTIONS.index(reduce_list[<span class="number">1</span>]),</span><br><span class="line">      OPT_REDUCTIONS.index(reduce_list[<span class="number">2</span>]),</span><br><span class="line">      OPT_REDUCTIONS.index(reduce_list[<span class="number">3</span>]),</span><br><span class="line">      BLOCK_SIZE=<span class="number">256</span>,</span><br><span class="line">  )</span><br></pre></td></tr></table></figure><p>使用 <code>triton.cdiv</code> 和一个 <code>grid</code> 函数计算内核的网格大小，调用 GPU 内核 <code>fused_scatter_reduce_forward_kernel</code> 来执行聚合操作。该内核会根据提供的 <code>inputs</code> 和 <code>index</code> 对不同的聚合操作进行计算。我们将在下面介绍<code>fused_scatter_reduce_forward_kernel</code>的具体实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Post-processing:</span></span><br><span class="line">  <span class="keyword">if</span> <span class="string">&#x27;mean&#x27;</span> <span class="keyword">in</span> reduce_slice_dict:</span><br><span class="line">      degree = inputs.new_zeros(dim_size)</span><br><span class="line">      degree.scatter_add_(<span class="number">0</span>, index, inputs.new_ones(index.numel()))</span><br><span class="line">      degree.clamp_(<span class="built_in">min</span>=<span class="number">1.0</span>)</span><br><span class="line">      tmp = out[:, reduce_slice_dict[<span class="string">&#x27;mean&#x27;</span>]]</span><br><span class="line">      tmp /= degree.view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">  <span class="keyword">if</span> <span class="string">&#x27;min&#x27;</span> <span class="keyword">in</span> reduce_slice_dict:</span><br><span class="line">      tmp = out[:, reduce_slice_dict[<span class="string">&#x27;min&#x27;</span>]]</span><br><span class="line">      tmp[tmp == <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)] = <span class="number">0.</span></span><br><span class="line">  <span class="keyword">if</span> <span class="string">&#x27;max&#x27;</span> <span class="keyword">in</span> reduce_slice_dict:</span><br><span class="line">      tmp = out[:, reduce_slice_dict[<span class="string">&#x27;max&#x27;</span>]]</span><br><span class="line">      tmp[tmp == <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)] = <span class="number">0.</span></span><br></pre></td></tr></table></figure><p><strong>mean</strong>：对于 <code>mean</code> 聚合操作，代码使用 <code>scatter_add_</code> 计算每个组的大小（即每个组的元素数量），然后将聚合结果除以组的大小。</p><p><strong>min</strong>：对于 <code>min</code> 聚合操作，将值为 <code>float(&#39;inf&#39;)</code> 的元素设置为 0。</p><p><strong>max</strong>：对于 <code>max</code> 聚合操作，将值为 <code>float(&#39;-inf&#39;)</code> 的元素设置为 0。</p><h4 id="fused-scatter-reduce-forward-kernel"><a href="#fused-scatter-reduce-forward-kernel" class="headerlink" title="_fused_scatter_reduce_forward_kernel"></a>_fused_scatter_reduce_forward_kernel</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_fused_scatter_reduce_forward_kernel</span>(<span class="params">inputs_ptr, index_ptr, out_ptr,</span></span><br><span class="line"><span class="params">                                         num_feats, num_reductions, numel,</span></span><br><span class="line"><span class="params">                                         REDUCE0, REDUCE1, REDUCE2, REDUCE3,</span></span><br><span class="line"><span class="params">                                         BLOCK_SIZE: tl.constexpr</span>):</span><br><span class="line">    pid = tl.program_id(axis=<span class="number">0</span>)</span><br><span class="line">    block_start = pid * BLOCK_SIZE</span><br><span class="line"></span><br><span class="line">    offsets = block_start + tl.arange(<span class="number">0</span>, BLOCK_SIZE)</span><br><span class="line">    mask = offsets &lt; numel</span><br><span class="line">    inputs = tl.load(inputs_ptr + offsets, mask=mask)</span><br><span class="line"></span><br><span class="line">    index_offsets = offsets // num_feats</span><br><span class="line">    index = tl.load(index_ptr + index_offsets, mask=mask)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># NOTE Triton does not support for-loops. As such, we cap the maximum</span></span><br><span class="line">    <span class="comment"># number of fused operations to `4` and unroll the loop.</span></span><br><span class="line">    <span class="comment"># TODO (matthias) Try to clean this up.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> REDUCE0 &gt; <span class="number">0</span>:</span><br><span class="line">        out_offsets = (num_feats * num_reductions) * index</span><br><span class="line">        out_offsets = out_offsets + (offsets % num_feats)</span><br><span class="line">        <span class="keyword">if</span> REDUCE0 == <span class="number">1</span>:  <span class="comment"># sum</span></span><br><span class="line">            tl.atomic_add(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">        <span class="keyword">elif</span> REDUCE0 == <span class="number">2</span>:  <span class="comment"># mean</span></span><br><span class="line">            tl.atomic_add(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">        <span class="keyword">elif</span> REDUCE0 == <span class="number">3</span>:  <span class="comment"># min</span></span><br><span class="line">            tl.atomic_min(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">        <span class="keyword">elif</span> REDUCE0 == <span class="number">4</span>:  <span class="comment"># max</span></span><br><span class="line">            tl.atomic_max(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> REDUCE1 &gt; <span class="number">0</span>:</span><br><span class="line">        out_offsets = (num_feats * num_reductions) * index</span><br><span class="line">        out_offsets = out_offsets + num_feats</span><br><span class="line">        out_offsets = out_offsets + (offsets % num_feats)</span><br><span class="line">        <span class="keyword">if</span> REDUCE1 == <span class="number">1</span>:  <span class="comment"># sum</span></span><br><span class="line">            tl.atomic_add(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">        <span class="keyword">elif</span> REDUCE1 == <span class="number">2</span>:  <span class="comment"># mean</span></span><br><span class="line">            tl.atomic_add(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">        <span class="keyword">elif</span> REDUCE2 == <span class="number">3</span>:  <span class="comment"># min</span></span><br><span class="line">            tl.atomic_min(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">        <span class="keyword">elif</span> REDUCE3 == <span class="number">4</span>:  <span class="comment"># max</span></span><br><span class="line">            tl.atomic_max(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> REDUCE2 &gt; <span class="number">0</span>:</span><br><span class="line">        out_offsets = (num_feats * num_reductions) * index</span><br><span class="line">        out_offsets = out_offsets + (<span class="number">2</span> * num_feats)</span><br><span class="line">        out_offsets = out_offsets + (offsets % num_feats)</span><br><span class="line">        <span class="keyword">if</span> REDUCE2 == <span class="number">1</span>:  <span class="comment"># sum</span></span><br><span class="line">            tl.atomic_add(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">        <span class="keyword">elif</span> REDUCE2 == <span class="number">2</span>:  <span class="comment"># mean</span></span><br><span class="line">            tl.atomic_add(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">        <span class="keyword">elif</span> REDUCE2 == <span class="number">3</span>:  <span class="comment"># min</span></span><br><span class="line">            tl.atomic_min(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">        <span class="keyword">elif</span> REDUCE2 == <span class="number">4</span>:  <span class="comment"># max</span></span><br><span class="line">            tl.atomic_max(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> REDUCE3 &gt; <span class="number">0</span>:</span><br><span class="line">        out_offsets = (num_feats * num_reductions) * index</span><br><span class="line">        out_offsets = out_offsets + (<span class="number">3</span> * num_feats)</span><br><span class="line">        out_offsets = out_offsets + (offsets % num_feats)</span><br><span class="line">        <span class="keyword">if</span> REDUCE3 == <span class="number">1</span>:  <span class="comment"># sum</span></span><br><span class="line">            tl.atomic_add(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">        <span class="keyword">elif</span> REDUCE3 == <span class="number">2</span>:  <span class="comment"># mean</span></span><br><span class="line">            tl.atomic_add(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">        <span class="keyword">elif</span> REDUCE3 == <span class="number">3</span>:  <span class="comment"># min</span></span><br><span class="line">            tl.atomic_min(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">        <span class="keyword">elif</span> REDUCE3 == <span class="number">4</span>:  <span class="comment"># max</span></span><br><span class="line">            tl.atomic_max(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>该函数 <strong><code>_fused_scatter_reduce_forward_kernel</code></strong> 是一个使用 <strong>Triton</strong> 库编写的 CUDA 内核，用于执行融合的散点聚合操作（scatter-reduce operations）。Triton 是一个专为深度学习模型优化的低级编程库，能够为 GPU 提供高效的内核编程支持。这个内核的主要目标是通过并行执行不同的聚合操作（如求和、均值、最大值和最小值），来提高计算效率，特别是在处理大规模数据时。</p><p><strong>并行化</strong>：该内核通过每个线程并行地处理不同的输入元素，并使用块级并行来加速计算。</p><p><strong>原子操作</strong>：使用原子加法、最小值和最大值操作，确保在多线程环境下进行聚合时不会出现竞争条件。</p><p><strong>Triton 的优势</strong>：Triton 提供的内存访问和原子操作机制，能够充分发挥 GPU 的并行计算能力，从而提高聚合操作的效率。</p><h5 id="计算程序-ID-和块起始位置"><a href="#计算程序-ID-和块起始位置" class="headerlink" title="计算程序 ID 和块起始位置"></a>计算程序 ID 和块起始位置</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pid = tl.program_id(axis=<span class="number">0</span>)</span><br><span class="line">block_start = pid * BLOCK_SIZE</span><br></pre></td></tr></table></figure><ul><li><code>pid</code> 是当前程序的 ID，根据每个线程块在程序中的位置进行分配。<code>block_start</code> 是当前块的起始位置，决定了本块内每个线程的处理区域。</li></ul><h5 id="计算每个线程处理的偏移量"><a href="#计算每个线程处理的偏移量" class="headerlink" title="计算每个线程处理的偏移量"></a>计算每个线程处理的偏移量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">offsets = block_start + tl.arange(<span class="number">0</span>, BLOCK_SIZE)</span><br><span class="line">mask = offsets &lt; numel</span><br><span class="line">inputs = tl.load(inputs_ptr + offsets, mask=mask)</span><br></pre></td></tr></table></figure><ul><li><code>offsets</code> 是当前线程块内各个线程处理的数据的索引。</li><li><code>mask</code> 用于确保线程不会访问超出数据范围的内存。</li><li><code>inputs</code> 是加载当前线程处理的数据块，它通过 <code>inputs_ptr + offsets</code> 从内存中读取数据。</li></ul><h5 id="计算对应的索引值"><a href="#计算对应的索引值" class="headerlink" title="计算对应的索引值"></a>计算对应的索引值</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index_offsets = offsets // num_feats</span><br><span class="line">index = tl.load(index_ptr + index_offsets, mask=mask)</span><br></pre></td></tr></table></figure><ul><li><code>index_offsets</code> 是通过将 <code>offsets</code> 除以 <code>num_feats</code> 得到的索引偏移量。</li><li><code>index</code> 是当前线程读取的索引值，指定了每个输入元素的目标组。</li></ul><h5 id="聚合操作的执行"><a href="#聚合操作的执行" class="headerlink" title="聚合操作的执行"></a>聚合操作的执行</h5><p>这部分代码根据传入的聚合操作类型（<code>REDUCE0</code>, <code>REDUCE1</code>, <code>REDUCE2</code>, <code>REDUCE3</code>）来执行不同的操作。对于每个操作，都会计算输出的偏移量并根据指定的聚合方式（如 sum、mean、min、max）执行相应的计算。</p><p>以 <code>REDUCE0</code> 为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> REDUCE0 &gt; <span class="number">0</span>:</span><br><span class="line">    out_offsets = (num_feats * num_reductions) * index</span><br><span class="line">    out_offsets = out_offsets + (offsets % num_feats)</span><br><span class="line">    <span class="keyword">if</span> REDUCE0 == <span class="number">1</span>:  <span class="comment"># sum</span></span><br><span class="line">        tl.atomic_add(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">    <span class="keyword">elif</span> REDUCE0 == <span class="number">2</span>:  <span class="comment"># mean</span></span><br><span class="line">        tl.atomic_add(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">    <span class="keyword">elif</span> REDUCE0 == <span class="number">3</span>:  <span class="comment"># min</span></span><br><span class="line">        tl.atomic_min(out_ptr + out_offsets, inputs, mask=mask)</span><br><span class="line">    <span class="keyword">elif</span> REDUCE0 == <span class="number">4</span>:  <span class="comment"># max</span></span><br><span class="line">        tl.atomic_max(out_ptr + out_offsets, inputs, mask=mask)</span><br></pre></td></tr></table></figure><ul><li><p>聚合操作的选择</p><p>根据 REDUCE0的值来选择聚合操作类型</p><ul><li><strong>sum</strong>：使用 <code>tl.atomic_add</code> 进行加法聚合。</li><li><strong>mean</strong>：使用 <code>tl.atomic_add</code> 进行加法，然后在后处理阶段进行均值计算。</li><li><strong>min</strong>：使用 <code>tl.atomic_min</code> 进行最小值聚合。</li><li><strong>max</strong>：使用 <code>tl.atomic_max</code> 进行最大值聚合。</li></ul></li></ul><p>每个聚合操作会计算相应的输出偏移量（<code>out_offsets</code>），并将结果存储在 <code>out_ptr</code> 指向的输出张量中。</p><p>类似的操作会对 <code>REDUCE1</code>, <code>REDUCE2</code>, 和 <code>REDUCE3</code> 进行处理，分别执行不同的聚合操作。</p><h5 id="聚合结果的更新"><a href="#聚合结果的更新" class="headerlink" title="聚合结果的更新"></a>聚合结果的更新</h5><p>每个聚合操作都会通过原子操作（如 <code>tl.atomic_add</code>, <code>tl.atomic_min</code>, <code>tl.atomic_max</code>）更新输出张量。原子操作确保在多个线程并发访问时，数据的一致性和正确性。</p><h3 id="2-5-softmax-csr"><a href="#2-5-softmax-csr" class="headerlink" title="2.5 softmax_csr()"></a>2.5 softmax_csr()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_csr</span>(<span class="params"></span></span><br><span class="line"><span class="params">    src: Tensor,</span></span><br><span class="line"><span class="params">    ptr: Tensor,</span></span><br><span class="line"><span class="params">    dim: <span class="built_in">int</span> = <span class="number">0</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; Tensor:</span><br><span class="line">   </span><br></pre></td></tr></table></figure><h4 id="功能描述-4"><a href="#功能描述-4" class="headerlink" title="功能描述"></a><strong>功能描述</strong></h4><p><code>softmax_csr</code> 是一个计算稀疏数据上 Softmax 的函数。给定一个张量 <code>src</code> 和一个 CSR（Compressed Sparse Row）格式的指示数组 <code>ptr</code>，该函数在指定维度 <code>dim</code> 上对每一组进行独立的 Softmax 计算。CSR 格式是一个常用的稀疏矩阵表示方法，适用于大规模稀疏矩阵的计算，能够节省内存并提高计算效率。</p><p>该函数的功能主要包括：</p><ul><li>根据 <code>ptr</code> 数组将数据 <code>src</code> 按照给定维度 <code>dim</code> 进行分组。</li><li>在每个组内，单独计算 Softmax。Softmax 的计算方式是在每个组内部进行归一化，以确保每组的元素和为 1。</li></ul><h4 id="输入参数-4"><a href="#输入参数-4" class="headerlink" title="输入参数"></a><strong>输入参数</strong></h4><p><strong>src (Tensor)</strong>: 输入张量，包含需要计算 Softmax 的数据。通常是一个二维或更高维的张量，其每一维的元素代表一个需要计算 Softmax 的数据点。<code>src</code> 必须是一个浮动点类型的张量（如 <code>float32</code> 或 <code>float64</code>），并且必须位于 CUDA 设备上。</p><p><strong>ptr (Tensor)</strong>: 用于指示分组的 CSR 指针数组。<code>ptr</code> 是一个一维张量，表示每一组的起始位置。其长度为 <code>N + 1</code>，其中 <code>N</code> 是数据中组的数量，<code>ptr[i]</code> 表示第 <code>i</code> 组的起始位置（包含）。<code>ptr[i + 1]</code> 表示第 <code>i</code> 组的结束位置。<code>ptr</code> 也必须位于 CUDA 设备上。</p><p><strong>dim (int, optional)</strong>: 指定 Softmax 计算的维度。默认值为 0，即在第一个维度上计算 Softmax。如果 <code>dim</code> 小于零，则通过 <code>dim + src.dim()</code> 计算一个合法的维度值。</p><h4 id="返回值-4"><a href="#返回值-4" class="headerlink" title="返回值"></a><strong>返回值</strong></h4><p>返回一个张量，其中每个组内的元素已经按 Softmax 计算进行归一化。结果的维度与输入张量 <code>src</code> 相同。</p><h4 id="具体实现-4"><a href="#具体实现-4" class="headerlink" title="具体实现"></a><strong>具体实现</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_csr</span>(<span class="params"></span></span><br><span class="line"><span class="params">    src: Tensor,</span></span><br><span class="line"><span class="params">    ptr: Tensor,</span></span><br><span class="line"><span class="params">    dim: <span class="built_in">int</span> = <span class="number">0</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; Tensor:</span><br><span class="line">    <span class="string">r&quot;&quot;&quot;Computes a sparsely evaluated softmax.</span></span><br><span class="line"><span class="string">    Given a value tensor :attr:`src`, this function first groups the values</span></span><br><span class="line"><span class="string">    along the given dimension :attr:`dim`, based on the indices specified via</span></span><br><span class="line"><span class="string">    :attr:`ptr`, and then proceeds to compute the softmax individually for</span></span><br><span class="line"><span class="string">    each group.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        src: The source tensor.</span></span><br><span class="line"><span class="string">        ptr: Groups defined by CSR representation.</span></span><br><span class="line"><span class="string">        dim: The dimension in which to normalize.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 确保 dim 是一个合法的维度索引</span></span><br><span class="line">    dim = dim + src.dim() <span class="keyword">if</span> dim &lt; <span class="number">0</span> <span class="keyword">else</span> dim</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用 PyTorch Geometric 提供的底层操作来计算稀疏 Softmax</span></span><br><span class="line">    <span class="keyword">return</span> torch.ops.pyg.softmax_csr(src, ptr, dim)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>维度处理</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dim = dim + src.dim() <span class="keyword">if</span> dim &lt; <span class="number">0</span> <span class="keyword">else</span> dim</span><br></pre></td></tr></table></figure><p>这行代码用于确保 <code>dim</code> 始终是一个合法的维度索引。如果 <code>dim</code> 是负数，它将根据 <code>src</code> 的维度大小进行调整，将其转换为一个正的有效维度索引。</p><p><strong>核心操作</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> torch.ops.pyg.softmax_csr(src, ptr, dim)</span><br></pre></td></tr></table></figure><p>这里调用了 PyTorch Geometric 库中的底层操作 <code>torch.ops.pyg.softmax_csr</code> 来执行稀疏 Softmax 计算。该操作会在 CSR 格式的数据上进行 Softmax 计算，并且每个组内会独立进行归一化。</p><h2 id="3-sampler"><a href="#3-sampler" class="headerlink" title="3. sampler"></a>3. sampler</h2><h4 id="3-1-neighbor-sample"><a href="#3-1-neighbor-sample" class="headerlink" title="3.1 neighbor_sample()"></a>3.1 neighbor_sample()</h4><h5 id="功能描述-5"><a href="#功能描述-5" class="headerlink" title="功能描述"></a>功能描述</h5><p>用于从图中的种子节点递归地采样邻居。可以通过设置不同的参数来调整采样方式，如采样数量、时间戳、边权重等。</p><h5 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">neighbor_sample</span>(<span class="params"></span></span><br><span class="line"><span class="params">    rowptr: Tensor,</span></span><br><span class="line"><span class="params">    col: Tensor,</span></span><br><span class="line"><span class="params">    seed: Tensor,</span></span><br><span class="line"><span class="params">    num_neighbors: <span class="type">List</span>[<span class="built_in">int</span>],</span></span><br><span class="line"><span class="params">    node_time: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    edge_time: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    seed_time: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    edge_weight: <span class="type">Optional</span>[Tensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    csc: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    replace: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    directed: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    disjoint: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    temporal_strategy: <span class="built_in">str</span> = <span class="string">&#x27;uniform&#x27;</span>,</span></span><br><span class="line"><span class="params">    return_edge_id: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[Tensor, Tensor, Tensor, <span class="type">Optional</span>[Tensor], <span class="type">List</span>[<span class="built_in">int</span>], <span class="type">List</span>[<span class="built_in">int</span>]]:</span><br></pre></td></tr></table></figure><p><code>rowptr</code>: 压缩存储格式的源节点索引（Tensor）。</p><p><code>col</code>: 目标节点索引（Tensor）。</p><p><code>seed</code>: 种子节点索引（Tensor）。</p><p><code>num_neighbors</code>: 每个节点在每次迭代中要采样的邻居数量（List[int]）。</p><p>其他可选参数（如 <code>node_time</code>, <code>edge_time</code>, <code>edge_weight</code> 等）用于控制采样的特性。</p><h5 id="返回值-5"><a href="#返回值-5" class="headerlink" title="返回值"></a>返回值</h5><p>返回一个元组，包含：</p><ul><li><code>row_indices</code>: 采样子图的源节点索引。</li><li><code>col_indices</code>: 采样子图的目标节点索引。</li><li><code>node_id</code>: 所有采样节点的原始节点索引。</li><li><code>edge_id</code>: 可选，返回原始图中的边索引。</li><li><code>num_nodes_per_hop</code>: 每次迭代采样的节点数量。</li><li><code>num_edges_per_hop</code>: 每次迭代采样的边数量。</li></ul><h5 id="具体实现-5"><a href="#具体实现-5" class="headerlink" title="具体实现"></a>具体实现</h5><p>该函数使用了 <code>torch.ops.pyg.neighbor_sample</code> 来实现图的邻居采样。输入的参数包括图的结构（通过 <code>rowptr</code> 和 <code>col</code> 表示），种子节点和采样配置。它支持多种采样方式（如有时间戳、采样替代等）。</p><h3 id="3-2-hetero-neighbor-sample"><a href="#3-2-hetero-neighbor-sample" class="headerlink" title="3.2 hetero_neighbor_sample()"></a>3.2 hetero_neighbor_sample()</h3><h5 id="功能描述-6"><a href="#功能描述-6" class="headerlink" title="功能描述"></a>功能描述</h5><p>类似于 <code>neighbor_sample</code>，但适用于异构图（包含多个节点类型和边类型）。该函数能够递归地从多个节点类型和边类型中进行邻居采样。</p><h5 id="输入-1"><a href="#输入-1" class="headerlink" title="输入"></a>输入</h5><p><code>rowptr_dict</code>, <code>col_dict</code>: 分别是每个边类型的源节点和目标节点索引字典（<code>EdgeType -&gt; Tensor</code>）。</p><p><code>seed_dict</code>: 种子节点的字典（<code>NodeType -&gt; Tensor</code>）。</p><p><code>num_neighbors_dict</code>: 每个边类型的邻居数量字典（<code>EdgeType -&gt; List[int]</code>）。</p><p>其他可选参数（如 <code>node_time_dict</code>, <code>edge_time_dict</code>, <code>edge_weight_dict</code> 等）用于控制异构图的采样。</p><h5 id="返回值-6"><a href="#返回值-6" class="headerlink" title="返回值"></a>返回值</h5><p>返回一个元组，包含：</p><ul><li><code>row_dict</code>: 每个边类型的源节点索引字典。</li><li><code>col_dict</code>: 每个边类型的目标节点索引字典。</li><li><code>node_id_dict</code>: 每个节点类型的采样节点索引字典。</li><li><code>edge_id_dict</code>: 可选，每个边类型的原始边索引字典。</li><li><code>num_nodes_per_hop_dict</code>: 每个边类型的每次迭代采样的节点数量字典。</li><li><code>num_edges_per_hop_dict</code>: 每个边类型的每次迭代采样的边数量字典。</li></ul><h5 id="具体实现-6"><a href="#具体实现-6" class="headerlink" title="具体实现"></a>具体实现</h5><p><strong>处理节点和边类型</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">src_node_types = &#123;k[<span class="number">0</span>] <span class="keyword">for</span> k <span class="keyword">in</span> rowptr_dict.keys()&#125;</span><br><span class="line">dst_node_types = &#123;k[-<span class="number">1</span>] <span class="keyword">for</span> k <span class="keyword">in</span> rowptr_dict.keys()&#125;</span><br><span class="line">node_types = <span class="built_in">list</span>(src_node_types | dst_node_types)</span><br><span class="line">edge_types = <span class="built_in">list</span>(rowptr_dict.keys())</span><br></pre></td></tr></table></figure><ul><li><code>src_node_types</code> 和 <code>dst_node_types</code> 提取图中所有源节点和目标节点的类型。</li><li><code>node_types</code> 和 <code>edge_types</code> 是所有节点类型和边类型的列表。</li></ul><p><strong>类型映射</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TO_REL_TYPE = &#123;key: <span class="string">&#x27;__&#x27;</span>.join(key) <span class="keyword">for</span> key <span class="keyword">in</span> edge_types&#125;</span><br><span class="line">TO_EDGE_TYPE = &#123;<span class="string">&#x27;__&#x27;</span>.join(key): key <span class="keyword">for</span> key <span class="keyword">in</span> edge_types&#125;</span><br></pre></td></tr></table></figure><ul><li><code>TO_REL_TYPE</code> 和 <code>TO_EDGE_TYPE</code> 分别为边类型和关系类型提供了映射，用于在函数中使用不同的表示形式。</li></ul><p><strong>格式化输入字典</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rowptr_dict = &#123;TO_REL_TYPE[k]: v <span class="keyword">for</span> k, v <span class="keyword">in</span> rowptr_dict.items()&#125;</span><br><span class="line">col_dict = &#123;TO_REL_TYPE[k]: v <span class="keyword">for</span> k, v <span class="keyword">in</span> col_dict.items()&#125;</span><br><span class="line">num_neighbors_dict = &#123;</span><br><span class="line">    TO_REL_TYPE[k]: v</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> num_neighbors_dict.items()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> edge_time_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    edge_time_dict = &#123;TO_REL_TYPE[k]: v <span class="keyword">for</span> k, v <span class="keyword">in</span> edge_time_dict.items()&#125;</span><br><span class="line"><span class="keyword">if</span> edge_weight_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    edge_weight_dict = &#123;</span><br><span class="line">        TO_REL_TYPE[k]: v</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> edge_weight_dict.items()</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><ul><li>将输入的 <code>rowptr_dict</code>, <code>col_dict</code>, <code>num_neighbors_dict</code> 等字典中的键转换为统一的格式，例如使用 <code>&#39;__&#39;.join(key)</code> 合并边类型的元组，生成统一的字符串表示。</li></ul><p><strong>调用 pyg-lib底层操作</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">out = torch.ops.pyg.hetero_neighbor_sample(  <span class="comment">#</span></span><br><span class="line">       node_types, edge_types, rowptr_dict, col_dict, seed_dict,</span><br><span class="line">       num_neighbors_dict, node_time_dict, edge_time_dict, seed_time_dict,</span><br><span class="line">       edge_weight_dict, csc, replace, directed, disjoint, temporal_strategy,</span><br><span class="line">       return_edge_id)</span><br><span class="line"></span><br><span class="line">   (row_dict, col_dict, node_id_dict, edge_id_dict, num_nodes_per_hop_dict,</span><br><span class="line">    num_edges_per_hop_dict) = out</span><br></pre></td></tr></table></figure><ul><li>最终，函数会调用 <code>torch.ops.pyg.hetero_neighbor_sample</code> 来执行异构图上的邻居采样操作。这个操作会根据图的结构和参数配置采样邻居，并返回相关的采样信息。</li></ul><p><strong>处理返回值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">row_dict = &#123;TO_EDGE_TYPE[k]: v <span class="keyword">for</span> k, v <span class="keyword">in</span> row_dict.items()&#125;</span><br><span class="line">  col_dict = &#123;TO_EDGE_TYPE[k]: v <span class="keyword">for</span> k, v <span class="keyword">in</span> col_dict.items()&#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> edge_id_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      edge_id_dict = &#123;TO_EDGE_TYPE[k]: v <span class="keyword">for</span> k, v <span class="keyword">in</span> edge_id_dict.items()&#125;</span><br><span class="line"></span><br><span class="line">  num_edges_per_hop_dict = &#123;</span><br><span class="line">      TO_EDGE_TYPE[k]: v</span><br><span class="line">      <span class="keyword">for</span> k, v <span class="keyword">in</span> num_edges_per_hop_dict.items()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (row_dict, col_dict, node_id_dict, edge_id_dict,</span><br><span class="line">          num_nodes_per_hop_dict, num_edges_per_hop_dict)</span><br></pre></td></tr></table></figure><ul><li>返回的结果包括采样后的子图的节点和边的索引信息。对于每种边类型，<code>row_dict</code> 和 <code>col_dict</code> 分别保存了源节点和目标节点的索引。</li><li>如果要求返回边 ID，<code>edge_id_dict</code> 会保存相应的边 ID。</li><li><code>num_nodes_per_hop_dict</code> 和 <code>num_edges_per_hop_dict</code> 分别记录了每一跳的节点数和边数。</li></ul><h3 id="3-3-subgraph"><a href="#3-3-subgraph" class="headerlink" title="3.3 subgraph()"></a>3.3 subgraph()</h3><h4 id="功能描述-7"><a href="#功能描述-7" class="headerlink" title="功能描述"></a>功能描述</h4><p>从原始图中返回一个子图，该子图包含指定节点的所有邻居节点及其边。</p><h4 id="输入-2"><a href="#输入-2" class="headerlink" title="输入"></a>输入</h4><p><code>rowptr</code>: 压缩存储格式的源节点索引（Tensor）。</p><p><code>col</code>: 目标节点索引（Tensor）。</p><p><code>nodes</code>: 要提取的节点索引（Tensor）。</p><p><code>return_edge_id</code>: 是否返回边的索引（Boolean）。</p><h4 id="返回值-7"><a href="#返回值-7" class="headerlink" title="返回值"></a>返回值</h4><p>返回一个元组，包含：</p><ul><li><code>row_indices</code>: 子图的源节点索引。</li><li><code>col_indices</code>: 子图的目标节点索引。</li><li><code>edge_id</code>: 可选，返回原始图中的边索引。</li></ul><h4 id="具体实现-7"><a href="#具体实现-7" class="headerlink" title="具体实现"></a>具体实现</h4><p>该函数通过 <code>torch.ops.pyg.subgraph</code> 来提取指定节点的子图。输入的参数包括图的边的表示方式（<code>rowptr</code> 和 <code>col</code>），以及需要提取的节点（<code>nodes</code>）。</p><h3 id="3-4-random-walk"><a href="#3-4-random-walk" class="headerlink" title="3.4 random_walk()"></a>3.4 random_walk()</h3><h4 id="功能描述-8"><a href="#功能描述-8" class="headerlink" title="功能描述"></a>功能描述</h4><p>从种子节点开始，按照指定的步长和控制参数进行随机游走，模拟了类似于 <code>node2vec</code> 算法的行为。</p><h4 id="输入-3"><a href="#输入-3" class="headerlink" title="输入"></a>输入</h4><p><code>rowptr</code>: 压缩存储格式的源节点索引（Tensor）。</p><p><code>col</code>: 目标节点索引（Tensor）。</p><p><code>seed</code>: 随机游走的起始节点（Tensor）。</p><p><code>walk_length</code>: 随机游走的步长（int）。</p><p><code>p</code>, <code>q</code>: 控制随机游走的策略（float）。</p><h4 id="返回值-8"><a href="#返回值-8" class="headerlink" title="返回值"></a>返回值</h4><p>返回一个Tensor，表示每个种子节点的随机游走结果，形状为 <code>[seed.size(0), walk_length + 1]</code>，其中每一行是一个游走路径。</p><h4 id="具体实现-8"><a href="#具体实现-8" class="headerlink" title="具体实现"></a>具体实现</h4><p>使用 <code>torch.ops.pyg.random_walk</code> 来执行随机游走。通过设置不同的控制参数 <code>p</code> 和 <code>q</code>，可以调整游走的策略（广度优先或深度优先）。</p><h2 id="4-partition"><a href="#4-partition" class="headerlink" title="4. partition"></a>4. partition</h2><h3 id="4-1metis"><a href="#4-1metis" class="headerlink" title="4.1metis()"></a>4.1metis()</h3><h4 id="功能描述-9"><a href="#功能描述-9" class="headerlink" title="功能描述"></a>功能描述</h4><p>该函数实现了基于 METIS 算法的图划分功能，METIS 是一个广泛应用于图划分的库，常用于将图分割成多个子图，以减少跨子图的边数。这个功能在深度图神经网络中非常重要，特别是在处理大型图时，能够提高计算效率和降低内存消耗。此函数实现了基于图的压缩表示（CSR 格式）来执行图划分。</p><h4 id="输入-4"><a href="#输入-4" class="headerlink" title="输入"></a>输入</h4><p><strong><code>rowptr</code></strong> (<code>Tensor</code>):<br>压缩的源节点索引，通常是图的 CSR（Compressed Sparse Row）格式中的 <code>rowptr</code> 部分，表示每一行的起始位置。</p><p><strong><code>col</code></strong> (<code>Tensor</code>):<br>目标节点索引，通常是图的 CSR 格式中的 <code>col</code> 部分，表示每行对应的目标节点。</p><p><strong><code>num_partitions</code></strong> (<code>int</code>):<br>图划分的子图数量，指定将图分成多少个子图。</p><p><strong><code>node_weight</code></strong> (<code>Optional[Tensor]</code>):<br>节点的权重。每个节点可能有一个与其重要性或计算负担相关的权重，这个权重会影响节点在图划分中的分配。如果未提供，默认权重为 1。</p><p><strong><code>edge_weight</code></strong> (<code>Optional[Tensor]</code>):<br>边的权重。每条边可能有一个权重，表示连接两个节点的“强度”。边权重较大的边通常希望保留在同一个分区中。默认值为 <code>None</code>，表示边权重均为 1。</p><p><strong><code>recursive</code></strong> (<code>bool</code>):<br>是否使用多级递归二分法（recursive bisection）来划分图。递归方法通过逐步将图划分成更小的部分，再进行进一步的划分，直到最终达到指定的分区数量。与此相对的是 K-分区方法，它直接将图分成指定数量的子图。</p><h4 id="返回值-9"><a href="#返回值-9" class="headerlink" title="返回值"></a>返回值</h4><p><strong><code>Tensor</code></strong>:<br>函数返回一个向量，其中每个元素代表图中每个节点所属的分区。这个向量的长度等于节点数量，每个节点的值表示该节点所属的子图编号（从 0 到 <code>num_partitions - 1</code>）。</p><h4 id="具体实现-9"><a href="#具体实现-9" class="headerlink" title="具体实现"></a>具体实现</h4><p>最终，函数通过调用 <code>torch.ops.pyg.metis</code> 来执行图的划分操作。这一操作是通过 PyTorch 的操作接口封装的，实际上是调用底层 METIS 库来完成图的划分任务。METIS 使用高效的图划分算法来尽量减少跨分区的边数。</p>]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图编码器聚类导向</title>
    <link href="/2024/11/04/%E5%9B%BE%E7%BC%96%E7%A0%81%E5%99%A8%E8%81%9A%E7%B1%BB%E5%AF%BC%E5%90%91/"/>
    <url>/2024/11/04/%E5%9B%BE%E7%BC%96%E7%A0%81%E5%99%A8%E8%81%9A%E7%B1%BB%E5%AF%BC%E5%90%91/</url>
    
    <content type="html"><![CDATA[<h3 id="1-传统-GAE-进行图聚类的流程"><a href="#1-传统-GAE-进行图聚类的流程" class="headerlink" title="1. 传统 GAE 进行图聚类的流程"></a>1. 传统 GAE 进行图聚类的流程</h3><p>传统的图自编码器（GAE）用于图聚类的基本流程如下：</p><ol><li><p><strong>输入图数据</strong>：给定一个图 $G &#x3D; (V, E)$其中 $V$ 是节点集合，$E$是边集合。图的结构用邻接矩阵 $A$ 表示，节点特征用特征矩阵 $X $表示。</p></li><li><p><strong>编码过程（图卷积编码器）</strong>：GAE 使用图卷积网络（GCN）对节点特征进行编码。通过几层图卷积操作，模型生成一个低维嵌入矩阵 $Z$，表示节点在低维空间的特征：<br>$$<br>Z&#x3D;GCN(X,A)<br>$$<br>这种低维嵌入矩阵 $Z$ 尽可能地保留节点之间的相似性。</p></li><li><p><strong>解码过程（重构邻接矩阵）</strong>：GAE 的目标是重构图的邻接关系，即预测节点之间的连接。解码过程通常使用内积解码器，将每对节点的嵌入向量进行内积计算，再经过 Sigmoid 函数，生成一个重构的邻接矩阵 $\hat{A}$：<br>$$<br>\hat{A}_{ij} &#x3D; \sigma(Z_i \cdot Z_j)<br>$$<br>其中 $Z_i$和 $Z_j$ 是节点 $i$ 和节点 $j$ 的低维表示，$\sigma$ 是 Sigmoid 函数，用于将内积结果映射到 $[0, 1]$的范围。</p></li><li><p><strong>训练目标（重构损失）</strong>：训练 GAE 的目标是最小化原始邻接矩阵 AAA 和重构邻接矩阵 $\hat{A}$ 之间的差异，通常使用二元交叉熵损失来度量：<br>$$<br>L &#x3D; -\sum_{i,j} \left( A_{ij} \log(\hat{A}<em>{ij}) + (1 - A</em>{ij}) \log(1 - \hat{A}_{ij}) \right)<br>$$<br>通过最小化这个损失，模型能够学习到节点之间的邻接关系，从而生成有效的节点嵌入。</p></li><li><p><strong>聚类</strong>：在训练完成后，利用嵌入矩阵 $Z$对节点进行聚类（通常使用 K-means 或高斯混合模型等聚类算法）。嵌入表示 $Z$ 保留了节点的结构信息，使得具有相似嵌入的节点更可能被分到同一类。</p></li></ol><h3 id="2-FR和FD的概念"><a href="#2-FR和FD的概念" class="headerlink" title="2. FR和FD的概念"></a>2. FR和FD的概念</h3><h4 id="2-1-FR-特征随机性"><a href="#2-1-FR-特征随机性" class="headerlink" title="2.1 FR(特征随机性)"></a>2.1 FR(特征随机性)</h4><p>FR 指的是模型在训练过程中由于标签的随机性或模型初始化的随机性，导致节点特征的表示包含噪声或随机成分。具体来说，这种随机性使得模型的嵌入表示可能偏离合理的聚类目标。例如，在初始阶段，模型的特征可能没有明显的分类结构，而只是一些无规律的特征。这种随机性会导致模型的特征表示出现随机投影现象，即特征在聚类时的准确性和一致性较低，影响了聚类的效果。</p><p><strong>直观理解</strong>：FR可以看作模型初期的“噪声”或“不确定性”，当模型在随机标签或没有清晰分类信息的情况下训练时，模型生成的特征很可能没有聚类相关的结构。这是因为在模型训练初期，学习的特征更多是随机的、不明确的。</p><h4 id="2-2-FD-特征漂移"><a href="#2-2-FD-特征漂移" class="headerlink" title="2.2 FD(特征漂移)"></a>2.2 FD(特征漂移)</h4><p>FD 是指在模型训练过程中，特征逐渐偏离原有数据的结构。这种漂移可能是因为模型在训练过程中不断更新嵌入特征，使得它们偏离了数据的原始图结构。例如，随着模型在图结构上的卷积层数增加，特征会逐渐平滑，使得不同节点的特征越来越相似，这在聚类任务中会造成不同类别之间的差异变小，影响聚类性能。</p><p><strong>直观理解</strong>：FD是模型训练后期的“特征收敛”现象。由于模型不断进行聚类优化，有可能会忽略原始的结构信息，使得特征变得过于平滑，丢失了聚类的辨别性。例如，假设模型的图卷积层数不断增加，最后可能导致所有节点的特征向量都非常接近，从而失去聚类的分界线。</p><h4 id="2-3-GAE模型中FR与FD的权衡"><a href="#2-3-GAE模型中FR与FD的权衡" class="headerlink" title="2.3 GAE模型中FR与FD的权衡"></a>2.3 GAE模型中FR与FD的权衡</h4><p>与用于欧几里得数据聚类的典型自动编码器模型不同，GAE模型在同一层级上执行聚类和重构（即在相同的网络层上）。作者研究了在不同层级上执行聚类和重构对FR和FD的影响。为此，考虑两种可能的情景。设 $\mathcal{NN}(d,d^{\prime},L)$表示一个包含若干全连接层的层级网络，定义为$f$，其中每一层的结构为<br>$$<br>\begin{array}{rcl}f&amp;:\mathbb{R}^d\to\mathbb{R}^{d’}\&amp;z\mapsto ReLU(W_l…ReLU(W_1z+b_1)…+b_l),\end{array}<br>$$<br>第一种情景是在最后一个图卷积层的顶部增加全连接编码层，并在最后编码层进行聚类，如图1所示。第二种情景是在最后一个图卷积层之后增加全连接解码层，并在最后解码层进行重构，如图2所示。因此，我们将一个典型的基于GAE的聚类模型与图1和图2中的两种版本在嵌入表示层级的FR和FD表现上进行比较。</p><p><img src="/%E5%9B%BE%E4%BA%8C.jpg" alt="在最后一层图卷积层顶部加全连接编码层，在最后的编码层进行聚类"></p><p><img src="/%E5%9B%BE%E4%B8%89.jpg" alt="在最后一层图卷积层之后加全连接解码层，在最后的解码层进行重构"></p><p>因此，我们将一个典型的基于GAE的聚类模型与图1和图2中的两种版本在嵌入表示层级的FR和FD表现上进行比较。</p><h4 id="2-4-不同情境下FR和FD的理论分析"><a href="#2-4-不同情境下FR和FD的理论分析" class="headerlink" title="2.4 不同情境下FR和FD的理论分析"></a>2.4 不同情境下FR和FD的理论分析</h4><p>定理1：给定两个具有相同图卷积层的GAE模型$Q_1$和$Q_2$。$Q_1$最小化公式(1)中的目标函数，$Q_2$最小化公式(2)中的损失函数，其中 $f \in \mathcal{NN}(d,d^{\prime},L)$且$d^{\prime}\ll d.$。设 $\tau_{1}^{*}$是函数 $f$的利普希茨常数，$\dot{\bar{Z}}<em>{i}&#x3D;(z</em>{jj^{\prime}}-z_{ij^{\prime}})<em>{j,j^{\prime}}\in\mathbb{R}^{N\times\hat{d}}$，$\zeta_i&#x3D;(\left|z_j-z_i\right|<em>2)<em>j\in\mathbb{R}^N$且 $a_i$是$ A$的第$ i$行。<br>$$<br>L</em>{\mathcal{Q}</em>{1}}&#x3D;L</em>{clus}(Z(\theta))+\gamma L_{bce}(\hat{A}(Z(\theta)), A^{self})\L_{\mathcal{Q}<em>{2}}&#x3D;L</em>{clus}(f(Z(\theta)))+\gamma L_{bce}(\hat{A}(Z(\theta)), A^{self})\<br>$$</p><p>$$<br>\begin{gathered}<br>\bullet \Lambda_{CD}^{\prime}(\mathcal{Q}<em>{2},z</em>{i})&#x3D;\Lambda_{FD}^{\prime}(\mathcal{Q}<em>{1},z</em>{i}). \<br>·If<br>\tau_{1}^{*}\leqslant\sqrt{\frac{(\bar{Z}<em>{i}^{T}a</em>{i}^{sup})^{T}(\bar{Z}<em>{i}^{T}a</em>{i}^{clus})}{(\zeta_{i}^{T} a_{i}^{sup})(\zeta_{i}^{T}a_{i}^{clus})}} \<br>\text{then}<br>\Lambda_{FR}^{\prime}(\mathcal{Q}<em>{2},z</em>{i})\leqslant\Lambda_{FR}^{\prime}(\mathcal{Q}<em>{1},z</em>{i}).<br>\end{gathered}<br>$$</p><p>定理1对应了了第一种情景，在最后一个图卷积层上添加了一组编码层，并在最后编码层上应用聚类损失。我们知道，减少利普希茨常数意味着更好的泛化能力。约束网络 $f$的利普希茨常数会导致相比初始的基于GAE的聚类模型产生更高的FR。此外，FD不受增加编码层的影响。因此得出结论：在解码操作独立的情况下增加编码层会增加FR而不影响FD。这个结果的直观解释是，重构损失的梯度不会反向传播至新增的编码层，因此聚类损失更容易受到随机投影的影响。</p><p>定理2：给定两个具有相同图卷积层的GAE模型 $Q_1$和 $Q_2$。$Q_1$最小化公式(1)中的损失函数，$Q_2$最小化公式(2)中的损失函数，其中是一个单$f \in \mathcal{NN}(d,d^{\prime},L)$射函数且$d^{\prime}\gg d$。设 $\tau_2^{<em>}$为 $f^{-1}:f(\mathbb{R}^d)\to\mathbb{R}^d$的利普希茨常数，$\bar{Z}<em>{i}^{‘}&#x3D;((\tilde{f(z</em>{j})})<em>{j^{\prime}}-(\tilde{f(z</em>{i})})<em>{j^{\prime}})</em>{j,j^{\prime}}\in\mathbb{R}^{N\times\tilde{d^{\prime}}}$，$\zeta_{i}^{‘}&#x3D;(\left|f(z_{j})-f(z_{i})\right|<em>{2})</em>{j}\in\mathbb{R}^{N}$，且 $a_i$是$A $的第$ i$行。<br>$$<br>\begin{aligned}<br>&amp;&amp;&amp;L_{\mathcal{Q}<em>{1}}&#x3D;L</em>{clus}(Z(\theta))+\gamma L_{bce}(\hat{A}(Z(\theta)), A^{self})\<br>&amp;&amp;&amp;L_{\mathcal{Q}<em>{2}}&#x3D;L</em>{clus}(Z(\theta))+\gamma L_{bce}(\hat{A}(f(Z(\theta))), A^{self})\<br>&amp;&amp;&amp;\bullet \Lambda_{FR}^{\prime}(\mathcal{Q}<em>{2},z</em>{i})&#x3D;\Lambda_{FR}^{\prime}(\mathcal{Q}<em>{1},z</em>{i}). \<br>&amp;&amp;&amp;If \<br>&amp;&amp;&amp;\tau_{2}^{</em>}\leqslant\sqrt{\frac{(\bar{Z}<em>{i}^{‘T}a</em>{i}^{sup})^{T}(\bar{Z}<em>{i}^{‘T}\tilde{a}</em>{i}^{self})}{(\zeta_{i}^{‘T} a_{i}^{sup})(\zeta_{i}^{‘T}\tilde{a}<em>{i}^{self})}}, \<br>&amp;&amp;&amp;\text{then} \<br>&amp;&amp;&amp;\Lambda</em>{FD}^{\prime}(\mathcal{Q}<em>{2},z</em>{i})\geqslant\Lambda_{FD}^{\prime}(\mathcal{Q}<em>{1},z</em>{i}).<br>\end{aligned}<br>$$<br>在定理2中，对应了第二种情景，其中在最后一个图卷积层上添加了一组解码层，并在最后解码层上应用重构损失。这个情况类似于典型的自动编码器，其中解码器具有多个层。根据定理3，约束$f^{-1}$的利普希茨常数会比初始的基于GAE的聚类模型导致更低的FD。直观上，当重构损失的梯度需要反向传播经过多层时，解码层会减弱FD的影响。（当重构损失的梯度需要反向传播经过多层解码层时，梯度会逐层衰减，导致嵌入层受到的重构影响减弱。)</p><h4 id="2-5-图卷积操作对FD的影响"><a href="#2-5-图卷积操作对FD的影响" class="headerlink" title="2.5 图卷积操作对FD的影响"></a>2.5 图卷积操作对FD的影响</h4><p>图卷积操作是典型自动编码器模型和GAE模型之间的主要区别。对于单层GCN层，特征传播规则可以表示为 $X^{(k+1)}&#x3D;\phi(\tilde{A}^{self}X^{(k)}W_{k})$（节点的特征与邻居特征加权和），设 $h$是一个聚合函数，使得$h^{sup}(x_{i})&#x3D;\sum_{j}\tilde{a}<em>{ij}^{sup}x</em>{j}$,是基于真实聚类$A^{sup}$分配计算的 $x_i$，是真实聚类的中心。$h^{self}(x_{i}) &#x3D; \sum_{j}\tilde{a}<em>{ij}^{self}x</em>{j}$,基于自监督邻接矩阵 $A^{self}$的$x_i$​邻居的中心。定义了一个函数 P来局部评估图滤波操作对聚类任务的影响。<br>$$<br>\mathcal{P}(x_i)&#x3D;|x_i-h^{sup}(x_i)|_2-|h^{self}(x_i)-h^{sup}(x_i)|<em>2. (12)<br>$$<br>如果 $\mathcal{P}(x</em>{i})\geqslant0$，我们说图滤波操作对节点$v_i$的聚类有正面影响。为了理解滤波操作对FD的影响，作者考虑了两种可能的情况。</p><p><strong>假设1</strong>：自监督邻接矩阵$A^{self}$表示的是具有少量误差的直接邻居关系，(相连的节点特征相近)</p><p><strong>假设2</strong>：假设节点$v_i$的直接邻居在一个训练良好的ReLU仿射层中激活了相同的神经元(相邻的节点激活相同的神经元)</p><p><strong>定理3</strong>：给定两个优化相同目标函数的模型$Q_1$和$Q_2$。$Q_1$具有一个由函数$f_{1}(X) &#x3D; ReLU(XW)$表示的单层全连接编码层。$Q_2$具有一个由函数A$f_{2}(X)&#x3D;Re\hat{L}U(\tilde{A}^{self}XW)$表示的单层图卷积层。</p><p>在假设1和假设2的条件下，我们有<br>$$<br>If \ \ \ \mathcal{P}(f_1(x_i))\geqslant0 \ \ \ then\ \ \  \Lambda_{FD}^{\prime}(\mathcal{Q}<em>2,x_i)\leqslant\Lambda</em>{FD}^{\prime}(\mathcal{Q}<em>1,x_i).<br>$$<br>在定理3中，研究了第一个情景，即将单层图卷积编码器与单层全连接编码器进行比较。证明依赖于$A^{self}$的两个合理性质。具体来说，根据定义我们知道$A^{self}$将每个节点连接到少数直接邻居，而与之相对的 $A^{sup}$将每个节点与同一真实聚类中的所有节点连接。在这些合理的假设下，定理4表明，如果图卷积操作对$v_i$的聚类有正面影响,在全连接层之前执行图卷积操作会增加节点$v_i$的FD。直观地来说，由于$A^{self}$仅考虑直接邻居（由于$A^{self}$的稀疏性）并保持了一些与聚类无关的链接。对于每一层，我们知道图卷积操作等效于最小化损失函数$L</em>{\mathcal{C}}(X^{(k)},\tilde{A}^{self})$(相邻节点的特征差距)，这意味着在同一层上FD的增加。</p><h3 id="3-作者提出的聚类导向方式的改进"><a href="#3-作者提出的聚类导向方式的改进" class="headerlink" title="3. 作者提出的聚类导向方式的改进"></a>3. 作者提出的聚类导向方式的改进</h3><h4 id="3-1-两个操作符"><a href="#3-1-两个操作符" class="headerlink" title="3.1 两个操作符"></a>3.1 两个操作符</h4><p>作者提出了两个操作符。操作符可以逐渐将通用的自监督图转换为聚类导向的图。第一个一个采样操作符Ξ，它可以触发一种针对FR的保护机制。更具体地说，Ξ可以延缓FR的快速发生。第二个操作符$\gamma$，它可以触发一种针对FD的修正机制，$r$通过逐渐将重构的图转变为聚类导向的图来抵消FD的影响。</p><h4 id="3-2-Ξ的详细设计"><a href="#3-2-Ξ的详细设计" class="headerlink" title="3.2 Ξ的详细设计"></a>3.2 Ξ的详细设计</h4><p>有人观察到在预训练阶段使用随机标签训练模型后再使用真实标签微调模型，无法逆转标签随机性带来的影响。因此作者的设计是基于一种保护机制，优先选择带有未被破坏标签的样本，然后才使用它们进行训练。采样技术直接在预训练阶段后启动，并利用两个强有力的标准来收集具有可靠聚类分配的足够数量的节点。</p><p>详细流程如下</p><h5 id="3-2-1-将硬聚类矩阵转换为软聚类矩阵"><a href="#3-2-1-将硬聚类矩阵转换为软聚类矩阵" class="headerlink" title="3.2.1 将硬聚类矩阵转换为软聚类矩阵"></a>3.2.1 将硬聚类矩阵转换为软聚类矩阵</h5><p>如果聚类结果已经是软聚类矩阵了，则直接使用否则根据以下公式将其转换为软聚类矩阵：<br>$$<br>p_{ij}’&#x3D;\frac{exp(-\frac{1}{2}\left(z_{i}-\mu_{j}\right)^{T}\Sigma_{j}^{-1}\left(z_{i}-\mu_{j}\right))}{\sum_{j&#x3D;1}^{K}exp(-\frac{1}{2}\left(z_{i}-\mu_{j}\right)^{T}\Sigma_{j}^{-1}\left(z_{i}-\mu_{j}\right))}<br>$$<br>其中$\mu_{j}$表示聚类簇$C_j^{clus}$的中心，$\Sigma_{j}$表示簇方差的对角矩阵。</p><h5 id="3-2-2-提取前二大置信分数"><a href="#3-2-2-提取前二大置信分数" class="headerlink" title="3.2.2 提取前二大置信分数"></a>3.2.2 提取前二大置信分数</h5><p>从第一步得到的矩阵中提取前二大置信度分数，设为$\lambda_i^1$,$\lambda_i^2$</p><h5 id="3-2-3可靠节点集构造"><a href="#3-2-3可靠节点集构造" class="headerlink" title="3.2.3可靠节点集构造"></a>3.2.3可靠节点集构造</h5><p>当第一置信分数超过超参数$\alpha_1$，且第一置信分数与第二置信分数之差大于$\frac{\alpha_1}{2}$时，判定为可靠节点，加入可靠节点集$\Omega(t)$。</p><p>以上为整个操作符Ξ的过程其算法如下，复杂度为$\mathcal{O}(\bar{N}K^{2}d)$：<br><img src="/%E7%AE%97%E6%B3%951.jpg" alt="算法1"></p><h4 id="3-3-gamma-的详细设计"><a href="#3-3-gamma-的详细设计" class="headerlink" title="3.3 $\gamma$的详细设计"></a>3.3 $\gamma$的详细设计</h4><p>由于原始图中相连节点并不一定属于同一个聚类，因此以重构为导向进行聚类会有FD问题，因此作者通过$\gamma$将重构目标逐步转化为聚类导向的代价。详细流程如下</p><h5 id="3-3-1-中心节点确定"><a href="#3-3-1-中心节点确定" class="headerlink" title="3.3.1 中心节点确定"></a>3.3.1 中心节点确定</h5><p>计算聚类$C_{j}^{clus}$中的平均特征向量$\tilde{\mu}_{j}$，选取特征向量与其最近邻的节点作为中心节点。</p><h5 id="3-3-2-更改图结构的连接性"><a href="#3-3-2-更改图结构的连接性" class="headerlink" title="3.3.2 更改图结构的连接性"></a>3.3.2 更改图结构的连接性</h5><p>基于原始图结构$A$构建一个新自监督图$A_{clus}^{self}$。首先将$\Omega$中的每个节点与其聚类中的中心节点相连，然后删除不同聚类的节点之间的边，最终得到$A_{clus}^{self}$包含$K$个子图，代表不同聚类。</p><p>算法2总结了整体的流程，算法复杂度为$\mathcal{O}(N(d+K)+|\mathcal{E}|(N+K))$。不$\gamma$仅仅可以应用于$\Omega$还可以用于整个节点集$V$，从而逐步减轻FD。</p><p><img src="/%E7%AE%97%E6%B3%952.jpg" alt="算法2"></p><h4 id="3-3-代码实现"><a href="#3-3-代码实现" class="headerlink" title="3.3 代码实现"></a>3.3 代码实现</h4><h5 id="3-3-1-Ξ操作符的实现"><a href="#3-3-1-Ξ操作符的实现" class="headerlink" title="3.3.1 Ξ操作符的实现"></a>3.3.1 Ξ操作符的实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_unconflicted_data_index</span>(<span class="params">emb, centers_emb, beta1, beta2</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    根据嵌入向量和聚类中心确定哪些数据点是“未冲突的”并且置信度较高，用于进一步的聚类训练。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    emb : np.array</span></span><br><span class="line"><span class="string">        数据节点的嵌入向量矩阵，每一行代表一个节点的嵌入。</span></span><br><span class="line"><span class="string">    centers_emb : np.array</span></span><br><span class="line"><span class="string">        聚类中心的嵌入向量矩阵，每一行代表一个聚类中心的嵌入。</span></span><br><span class="line"><span class="string">    beta1 : float</span></span><br><span class="line"><span class="string">        第一个置信度阈值，数据点的最高置信度值需要大于此值。</span></span><br><span class="line"><span class="string">    beta2 : float</span></span><br><span class="line"><span class="string">        第二个置信度差异阈值，最高置信度与次高置信度的差异需要大于此值。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    unconf_indices : np.array</span></span><br><span class="line"><span class="string">        满足置信度条件的未冲突节点的索引数组。</span></span><br><span class="line"><span class="string">    conf_indices : np.array</span></span><br><span class="line"><span class="string">        不满足置信度条件的冲突节点的索引数组。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    unconf_indices = []  <span class="comment"># 存储符合条件的未冲突节点索引</span></span><br><span class="line">    conf_indices = []    <span class="comment"># 存储不符合条件的冲突节点索引</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算嵌入数据点与聚类中心的相似度矩阵 q</span></span><br><span class="line">    q = q_mat(emb, centers_emb, alpha=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化置信度数组，用于存储每个节点的最高和次高置信度</span></span><br><span class="line">    confidence1 = np.zeros((q.shape[<span class="number">0</span>],))</span><br><span class="line">    confidence2 = np.zeros((q.shape[<span class="number">0</span>],))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对相似度矩阵 q 中的每一行（每个节点）按置信度排序</span></span><br><span class="line">    a = np.argsort(q, axis=<span class="number">1</span>)  <span class="comment"># 对每个节点的聚类相似度排序，返回索引</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历每个节点，提取最高和次高置信度</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(q.shape[<span class="number">0</span>]):</span><br><span class="line">        confidence1[i] = q[i, a[i, -<span class="number">1</span>]]  <span class="comment"># 最高置信度值</span></span><br><span class="line">        confidence2[i] = q[i, a[i, -<span class="number">2</span>]]  <span class="comment"># 次高置信度值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据 beta1 和 beta2 判断是否为未冲突节点</span></span><br><span class="line">        <span class="comment"># 条件1：最高置信度值 &gt; beta1</span></span><br><span class="line">        <span class="comment"># 条件2：最高置信度与次高置信度的差值 &gt; beta2</span></span><br><span class="line">        <span class="keyword">if</span> (confidence1[i]) &gt; beta1 <span class="keyword">and</span> (confidence1[i] - confidence2[i]) &gt; beta2:</span><br><span class="line">            unconf_indices.append(i)  <span class="comment"># 满足条件，加入未冲突节点索引</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conf_indices.append(i)    <span class="comment"># 不满足条件，加入冲突节点索引</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将结果转为 numpy 数组格式</span></span><br><span class="line">    unconf_indices = np.asarray(unconf_indices, dtype=<span class="built_in">int</span>)</span><br><span class="line">    conf_indices = np.asarray(conf_indices, dtype=<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> unconf_indices, conf_indices</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="3-3-2-gamma-操作符的实现"><a href="#3-3-2-gamma-操作符的实现" class="headerlink" title="3.3.2 $\gamma$操作符的实现"></a>3.3.2 $\gamma$操作符的实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update_graph</span>(<span class="params">self, adj, labels, emb, unconf_indices, conf_indices</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    更新图的邻接矩阵，使其更加聚类导向，通过增加和删除边的操作来实现。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    adj : scipy.sparse.csr_matrix</span></span><br><span class="line"><span class="string">        原始邻接矩阵，表示图的连接结构。</span></span><br><span class="line"><span class="string">    labels : np.array</span></span><br><span class="line"><span class="string">        图中每个节点的标签。</span></span><br><span class="line"><span class="string">    emb : torch.Tensor</span></span><br><span class="line"><span class="string">        每个节点的嵌入向量。</span></span><br><span class="line"><span class="string">    unconf_indices : np.array</span></span><br><span class="line"><span class="string">        未冲突节点的索引数组，这些节点的聚类置信度较高。</span></span><br><span class="line"><span class="string">    conf_indices : np.array</span></span><br><span class="line"><span class="string">        冲突节点的索引数组，这些节点的聚类置信度较低。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    adj : scipy.sparse.csr_matrix</span></span><br><span class="line"><span class="string">        更新后的邻接矩阵。</span></span><br><span class="line"><span class="string">    adj_label : torch.sparse.FloatTensor</span></span><br><span class="line"><span class="string">        用于训练的邻接矩阵标签。</span></span><br><span class="line"><span class="string">    weight_tensor : torch.Tensor</span></span><br><span class="line"><span class="string">        权重张量，用于损失计算。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测每个节点的聚类标签</span></span><br><span class="line">    y_pred = <span class="variable language_">self</span>.predict(emb)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取未冲突节点的嵌入</span></span><br><span class="line">    emb_unconf = emb[unconf_indices]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保将邻接矩阵 adj 转换为稀疏矩阵的 CSR 格式</span></span><br><span class="line">    <span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csr_matrix</span><br><span class="line">    adj = csr_matrix(adj.tolil()).tocsr()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据未冲突节点生成的聚类中心找到最接近的未冲突节点索引</span></span><br><span class="line">    idx = unconf_indices[<span class="variable language_">self</span>.generate_centers(emb_unconf)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历每个未冲突节点的索引和它的中心节点索引</span></span><br><span class="line">    <span class="keyword">for</span> i, k <span class="keyword">in</span> <span class="built_in">enumerate</span>(unconf_indices):</span><br><span class="line">        adj_k = adj[k, :]  <span class="comment"># 获取 k 行，即节点 k 的所有邻接节点</span></span><br><span class="line">        adj_k_indices = adj_k.indices  <span class="comment"># 获取邻接节点的索引</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果中心节点不在 k 的邻接节点中，且 k 和中心节点属于同一聚类，则增加一条边</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> np.isin(idx[i], adj_k_indices) <span class="keyword">and</span> (y_pred[k] == y_pred[idx[i]]):</span><br><span class="line">            adj[k, idx[i]] = <span class="number">1</span>  <span class="comment"># 在节点 k 和中心节点之间添加边</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历 k 的邻接节点 j</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> adj_k_indices:</span><br><span class="line">            <span class="comment"># 如果 j 是未冲突节点，并且 k 和 j 不属于同一聚类，删除边</span></span><br><span class="line">            <span class="keyword">if</span> np.isin(j, unconf_indices) <span class="keyword">and</span> np.isin(idx[i], adj_k_indices) <span class="keyword">and</span> (y_pred[k] != y_pred[j]):</span><br><span class="line">                adj[k, j] = <span class="number">0</span>  <span class="comment"># 删除节点 k 和 j 之间的边</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将邻接矩阵 adj 保证为 CSR 格式，便于进一步计算</span></span><br><span class="line">    adj = adj.tocsr()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建训练标签的邻接矩阵，包括自环</span></span><br><span class="line">    adj_label = adj + sp.eye(adj.shape[<span class="number">0</span>])</span><br><span class="line">    adj_label = sparse_to_tuple(adj_label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转换邻接矩阵标签为稀疏张量格式，以便在 PyTorch 中处理</span></span><br><span class="line">    adj_label = torch.sparse.FloatTensor(</span><br><span class="line">        torch.LongTensor(adj_label[<span class="number">0</span>].T),</span><br><span class="line">        torch.FloatTensor(adj_label[<span class="number">1</span>]),</span><br><span class="line">        torch.Size(adj_label[<span class="number">2</span>])</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建权重张量用于损失计算</span></span><br><span class="line">    weight_mask = adj_label.to_dense().view(-<span class="number">1</span>) == <span class="number">1</span>  <span class="comment"># 标记边的位置</span></span><br><span class="line">    weight_tensor = torch.ones(weight_mask.size(<span class="number">0</span>))</span><br><span class="line">    pos_weight_orig = <span class="built_in">float</span>(adj.shape[<span class="number">0</span>] * adj.shape[<span class="number">0</span>] - adj.<span class="built_in">sum</span>()) / adj.<span class="built_in">sum</span>()  <span class="comment"># 计算正样本的权重</span></span><br><span class="line">    weight_tensor[weight_mask] = pos_weight_orig  <span class="comment"># 为正样本赋权</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> adj, adj_label, weight_tensor</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>图编码器</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>卷积神经网络</title>
    <link href="/2024/11/01/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/11/01/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><h3 id="1-卷积"><a href="#1-卷积" class="headerlink" title="1. 卷积"></a>1. 卷积</h3><h4 id="1-1-从全连接层到卷积"><a href="#1-1-从全连接层到卷积" class="headerlink" title="1.1 从全连接层到卷积"></a>1.1 从全连接层到卷积</h4><p>多层感知机在面对不能预先假设任何与特征交互相关的先验结构时，可能是一个很好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。</p><p>例如，在之前猫狗分类的例子中：假设我们有一个足够充分的照片数据集，数据集中是拥有标注的照片，每张照片具有百万级像素，这意味着网络的每次输入都有一百万个维度。即使将隐藏层维度降低到1000，这个全连接层也将有$10^6×10^3&#x3D;10^9$个参数。 </p><p> 然而，如今人类和机器都能很好地区分猫和狗：这是因为图像中本就拥有丰富的结构，而这些结构可以被人类和机器学习模型使用。 <em>卷积神经网络</em>（convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法。</p><h4 id="1-2-卷积神经网络的两个原则"><a href="#1-2-卷积神经网络的两个原则" class="headerlink" title="1.2 卷积神经网络的两个原则"></a>1.2 卷积神经网络的两个原则</h4><ol><li><em>平移不变性</em>（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</li><li><em>局部性</em>（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</li></ol><h3 id="2-图像卷积"><a href="#2-图像卷积" class="headerlink" title="2. 图像卷积"></a>2. 图像卷积</h3>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模型初始化</title>
    <link href="/2024/10/29/%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <url>/2024/10/29/%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="数值稳定性和模型初始化"><a href="#数值稳定性和模型初始化" class="headerlink" title="数值稳定性和模型初始化"></a>数值稳定性和模型初始化</h2><p>始化方案的选择在神经网络学习中起着举足轻重的作用， 它对保持数值稳定性至关重要。 我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快。 糟糕选择可能会导致我们在训练时遇到梯度爆炸或梯度消失。</p><h3 id="1-梯度消失和梯度爆炸"><a href="#1-梯度消失和梯度爆炸" class="headerlink" title="1. 梯度消失和梯度爆炸"></a>1. 梯度消失和梯度爆炸</h3><ul><li>梯度爆炸：参数更新过大，破坏了模型的稳定收敛。</li><li>梯度消失：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。</li></ul><h4 id="1-1-梯度消失"><a href="#1-1-梯度消失" class="headerlink" title="1.1 梯度消失"></a>1.1 梯度消失</h4><p>sigmoid函数$\sigma(x) &#x3D; \frac{1}{1 + e^{-x}}$图像如下：</p><p><img src="/sigmoid.png" alt="sigmoid"></p><p>当sigmoid函数的输入很大或是很小时，它的梯度都会消失。 此外，当反向传播通过许多层时，除非我们在刚刚好的地方， 这些地方sigmoid函数的输入接近于零，否则整个乘积的梯度可能会消失。 当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度。 事实上，这个问题曾经困扰着深度网络的训练。 因此，更稳定的ReLU系列函数已经成为从业者的默认选择（虽然在神经科学的角度看起来不太合理）。</p><h4 id="1-2-梯度爆炸"><a href="#1-2-梯度爆炸" class="headerlink" title="1.2 梯度爆炸"></a>1.2 梯度爆炸</h4><p>深层的神经网络不断输入输出，相当于不断做矩阵乘法，如果不加限制，那么很容易会产生指数爆炸。</p><h3 id="2-参数初始化"><a href="#2-参数初始化" class="headerlink" title="2. 参数初始化"></a>2. 参数初始化</h3><h4 id="2-1-默认初始化"><a href="#2-1-默认初始化" class="headerlink" title="2.1 默认初始化"></a>2.1 默认初始化</h4><p>使用正态分布来初始化权重值。如果我们不指定初始化方法， 框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。</p><h4 id="2-2-Xavier初始化"><a href="#2-2-Xavier初始化" class="headerlink" title="2.2 Xavier初始化"></a>2.2 Xavier初始化</h4><p>Xavier初始化的主要思想是，保持每一层的输入和输出在正向传播和反向传播中均值方差尽可能的不变，这需要同时满足$n_{in}\sigma^2&#x3D;1$,$n_{out}\sigma^2&#x3D;1$。显然我们不可能同时满足这两个条件， Xavier采取了一种折中的方法，具体地：<br>$$<br>W \sim \mathcal{N}\left(0, \frac{2}{n_{in}+n_{out}}\right)<br>$$</p><p>$$<br>W \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}\right)<br>$$</p><p>Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响。尽管在上述数学推理中，“不存在非线性”的假设在神经网络中很容易被违反， 但Xavier初始化方法在实践中被证明是有效的。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>正则化</title>
    <link href="/2024/10/29/%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <url>/2024/10/29/%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="1-为什么要正则化"><a href="#1-为什么要正则化" class="headerlink" title="1. 为什么要正则化"></a>1. 为什么要正则化</h3><p>正则化主要用于处理过拟合，即防止模型单纯记住了所有的情况，在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。</p><h3 id="2-正则化方法"><a href="#2-正则化方法" class="headerlink" title="2. 正则化方法"></a>2. 正则化方法</h3><h4 id="2-1-权重衰减"><a href="#2-1-权重衰减" class="headerlink" title="2.1 权重衰减"></a>2.1 权重衰减</h4><p>权重衰减（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为$L_2$​正则化。其公式如下<br>$$<br>|\mathbf{x} |<em>2 &#x3D; \sqrt{\sum</em>{i&#x3D;1}^{n} x_i^2}<br>$$<br>为了惩罚权重向量的大小， 我们通过正则化常数$\lambda$在损失函数中添加${|\mathbf{w} |_2^2}$​：<br>$$<br>L(w,b) &#x3D; L_0(w,b) + \frac{\lambda}{2} |w|_2^2<br>$$<br>从整体来看，通过$L_2$正则化，要想使得loss足够小，那么 $|w|_2^2$需要足够小，从而达到权重衰减的效果，从而可以防止模型过于复杂。</p><p>是否对相应的偏置b2进行惩罚在不同的实践中会有所不同， 在神经网络的不同层中也会有所不同。 通常，网络输出层的偏置项不会被正则化。</p><h3 id="2-2-暂退法-Dropout"><a href="#2-2-暂退法-Dropout" class="headerlink" title="2.2 暂退法(Dropout)"></a>2.2 暂退法(Dropout)</h3><p>当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。</p><p>效果如下图所示。 当我们将暂退法应用到隐藏层，以$p$的概率将隐藏单元置为零时， 结果可以看作一个只包含原始神经元子集的网络。 比如在中，删除了$h_2$和$h_5$， 因此输出的计算不再依赖于$h_2$或$h_5$，并且它们各自的梯度在执行反向传播时也会消失。 这样，输出层的计算不能过度依赖于$(h_1, \ldots, h_5)$的任何一个元素。</p><p><img src="/dropout.png" alt="dropout前后的MLP"></p><p>具体地在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值$h$以暂退概率$p$由随机变量$h^′$替换，如下所示：<br>$$<br>h^′ &#x3D; \frac{1}{1-p} \cdot (h \odot r)<br>$$<br>通常，我们在测试时不用暂退法。 给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。 然而也有一些例外：一些研究人员在测试时使用暂退法， 用于估计神经网络预测的“不确定性”： 如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据集划分</title>
    <link href="/2024/10/28/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86/"/>
    <url>/2024/10/28/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86/</url>
    
    <content type="html"><![CDATA[<h2 id="1-数据集划分"><a href="#1-数据集划分" class="headerlink" title="1. 数据集划分"></a>1. 数据集划分</h2><p>我们通常将原始数据集分为三个部分：</p><ol><li>训练集：训练模型</li><li>验证集：选择模型</li><li>测试集：评估模型</li></ol><p>训练集顾名思义用于训练，其用来训练当前模型下最优的参数:$w,d$。验证集用于选择模型，根据结果调整超参数，例如层数、每一层的神经元个数。测试集用于评估模型，理论上来说测试集使用一次后就要丢弃。而实际上由于数据比较珍贵，我们难以做到抽出一部分数据作为测试集用过一次后就丢弃，因此大多数情况下：我们实际上是在使用应该被正确地称为训练数据和验证数据的数据集， 并没有真正的测试数据集。</p><h2 id="2-数据集划分方法"><a href="#2-数据集划分方法" class="headerlink" title="2. 数据集划分方法"></a>2. 数据集划分方法</h2><h3 id="2-1-留出法"><a href="#2-1-留出法" class="headerlink" title="2.1 留出法"></a>2.1 留出法</h3><p>1.如果数据比较少：</p><p>只划分训练集和验证集则为：70%验证集，30%验证集；</p><p>划分训练集、验证集和测试集则为：60%训练集，20%验证集，20%测试集。</p><p>2.数据比较多：</p><p>只需要取一小部分当做测试集和验证集，其他的都当做训练集。</p><p>然后使用训练集来生成模型，验证集来选择模型，最后用测试集来测试模型的正确率和误差，以验证模型的有效性。</p><h3 id="2-2-交叉验证法-Cross-Validation"><a href="#2-2-交叉验证法-Cross-Validation" class="headerlink" title="2.2 交叉验证法(Cross Validation)"></a>2.2 交叉验证法(Cross Validation)</h3><p>如果数据集足够小，我们甚至可能无法提供足够的数据来构成一个合适的验证集(实际上这种情况比较少)。这个问题的一个流行的解决方案是采用$K$折交叉验证。 这里，原始训练数据被分成$K$个不重叠的子集。 然后执行$K$次模型训练和验证，每次在$K−1$个子集上进行训练， 并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。 最后，通过对$K$次实验的结果取平均来估计训练和验证误差。</p><p>这样相当于每一次都重新初始化$w,b$, 所以并不会存在验证集也参与训练的可能。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多层感知机</title>
    <link href="/2024/10/27/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <url>/2024/10/27/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h2 id="1-多层感知机"><a href="#1-多层感知机" class="headerlink" title="1. 多层感知机"></a>1. 多层感知机</h2><p>之前的softmax回归是建立在线性模型的背景之下的，相当于没有隐藏层的神经网络。</p><h3 id="1-1-隐藏层"><a href="#1-1-隐藏层" class="headerlink" title="1.1 隐藏层"></a>1.1 隐藏层</h3><p>在softmax回归中，模型通过单个仿射变换将输入直接映射到了输出，这是建立在线性关系的基础上的。而显然对大部分事情来说，线性这一假设往往太过于简单。</p><h4 id="1-1-1-不符合线性的例子"><a href="#1-1-1-不符合线性的例子" class="headerlink" title="1.1.1 不符合线性的例子"></a>1.1.1 不符合线性的例子</h4><p>我们很容易想到一些不符合线性关系的例子，例如人体温度基于死亡率，这不是单调的。收入与还款概率，显然不是线性的。而我们又很难直接把握这一关系的大概样子，因此对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。</p><h4 id="1-1-2-加入隐藏层"><a href="#1-1-2-加入隐藏层" class="headerlink" title="1.1.2 加入隐藏层"></a>1.1.2 加入隐藏层</h4><p>将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前$L−1$层看作表示，把最后一层看作线性预测器,这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。</p><p><img src="/mlp.png" alt="具有5个隐藏单元的MLP"></p><h4 id="1-1-3-从线性到非线性"><a href="#1-1-3-从线性到非线性" class="headerlink" title="1.1.3 从线性到非线性"></a>1.1.3 从线性到非线性</h4><p>如果只是简单计算：<br>$$<br>H&#x3D;XW^{(1)}+b^{(1)}<br>$$<br>$$<br>O&#x3D;HW^{(2)}+b^{(2)}<br>$$</p><p>显然这只是两层线性的嵌套，因此如果只是这样还是相当于线性模型，因此我们需要一个非线性函数，激活函数(activation function)$\sigma$。<br>$$<br>H&#x3D;\sigma(XW^{(1)}+b^{(1)})<br>$$<br>$$<br>O&#x3D;HW^{(2)}+b^{(2)}<br>$$</p><p>一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型。为了构建更通用的多层感知机， 我们可以继续堆叠这样的隐藏层， 例如$H^{(1)}&#x3D;σ_1(XW^{(1)}+b^{(1)})$和$H^{(2)}&#x3D;σ_2(H^{(1)}W^{(2)}+b^{(2)})$， 一层叠一层，从而产生更有表达能力的模型。</p><h3 id="1-2-激活函数"><a href="#1-2-激活函数" class="headerlink" title="1.2 激活函数"></a>1.2 激活函数</h3><p><em>激活函数</em>（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。 大多数激活函数都是非线性的。 由于激活函数是深度学习的基础，下面简要介绍一些常见的激活函数。</p><h4 id="1-2-1-ReLU函数"><a href="#1-2-1-ReLU函数" class="headerlink" title="1.2.1 ReLU函数"></a>1.2.1 ReLU函数</h4><p>最受欢迎的激活函数是<em>修正线性单元</em>（Rectified linear unit，<em>ReLU</em>）， 因为它实现简单，同时在各种预测任务中表现良好。<br>$$<br>\text{ReLU}(x) &#x3D; \max(0, x)<br>$$<br>通俗地说，ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。</p><h4 id="1-2-2-sigmoid函数"><a href="#1-2-2-sigmoid函数" class="headerlink" title="1.2.2 sigmoid函数"></a>1.2.2 sigmoid函数</h4><p>对于一个定义域在$R$中的输入， <em>sigmoid函数</em>将输入变换为区间$(0, 1)$上的输出。 因此，sigmoid通常称为<em>挤压函数</em>（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：<br>$$<br>\sigma(x) &#x3D; \frac{1}{1 + e^{-x}}<br>$$<br>当我们想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数 （sigmoid可以视为softmax的特例）。 然而，sigmoid在隐藏层中已经较少使用， 它在大部分时候被更简单、更容易训练的ReLU所取代。 在后面关于循环神经网络的章节中，我们将描述利用sigmoid单元来控制时序信息流的架构。</p><h4 id="1-2-3-tanh函数"><a href="#1-2-3-tanh函数" class="headerlink" title="1.2.3 tanh函数"></a>1.2.3 tanh函数</h4><p>与sigmoid函数类似， tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。 tanh函数的公式如下：<br>$$<br>\tanh(x) &#x3D; \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}<br>$$</p><h2 id="2-多层感知机实现"><a href="#2-多层感知机实现" class="headerlink" title="2 多层感知机实现"></a>2 多层感知机实现</h2><p>在上次softmax对图片做分类的情景下使用MLP，相当于在softmax回归基础上添加了一层隐藏层。</p><h3 id="2-1-数据集"><a href="#2-1-数据集" class="headerlink" title="2.1 数据集"></a>2.1 数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line">d2l.train_ch3()</span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><h3 id="2-2-初始化"><a href="#2-2-初始化" class="headerlink" title="2.2 初始化"></a>2.2 初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">256</span>), nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><p>平展层将图片平展成784向量，然后一个隐藏层加relu激活函数最后是输出层。</p><h3 id="2-3-训练"><a href="#2-3-训练" class="headerlink" title="2.3 训练"></a>2.3 训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size, lr, num_epochs = <span class="number">256</span>, <span class="number">0.1</span>, <span class="number">10</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>softmax回归</title>
    <link href="/2024/10/26/softmax%E5%9B%9E%E5%BD%92/"/>
    <url>/2024/10/26/softmax%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h2 id="1-softmax回归原理"><a href="#1-softmax回归原理" class="headerlink" title="1. softmax回归原理"></a>1. softmax回归原理</h2><h3 id="1-1-分类和回归"><a href="#1-1-分类和回归" class="headerlink" title="1.1 分类和回归"></a>1.1 分类和回归</h3><p><strong>分类</strong>：分类任务的目标是预测一个<strong>离散的类别标签</strong>。例如，将图片分为猫和狗，预测邮件是垃圾邮件还是正常邮件等。类别标签通常是有限的，常见的标签形式为整数（如 0, 1, 2）或具体的分类名称（如“猫”，“狗”）。</p><p><strong>回归</strong>：回归任务的目标是预测一个<strong>连续的数值</strong>。例如，预测房价、股票价格、温度等。在回归问题中，输出值可以是任意实数。</p><h3 id="1-2-网络架构"><a href="#1-2-网络架构" class="headerlink" title="1.2 网络架构"></a>1.2 网络架构</h3><p>为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。</p><p>在上述例子（猫，鸡，狗）中，我们假设对于一个样本有4个特征，因此对于原始的样本其维度为[n, 4]，我们要将其最终映射到[n, 3]，其中“3”表示one-hot编码，例如我们可以假定<br>(1,0,0)对应于“狗”，(0,1,0)对应于“鸡”，(0,0,1)对应于“狗”。所以我们需要的<br>$w$维度为[3, 4], $b$的维度为[3, 1]，如下图所示，<br>$o_1,o_2,o_3$表示类别</p><p><img src="/softmax.jpg"></p><h3 id="1-3-softmax函数"><a href="#1-3-softmax函数" class="headerlink" title="1.3 softmax函数"></a>1.3 softmax函数</h3><p>Softmax 是一种常用的函数，用于多分类问题中将模型的输出值（logits）转换为概率分布。它能够将任意实数的输出值映射到一个区间为 <code>[0, 1]</code> 的概率空间，并且保证这些概率之和为 <code>1</code>。下面是对 Softmax 的详细介绍：</p><p>对于给定的输入向量 $z &#x3D; [z_1, z_2, \dots, z_n]$，Softmax 函数的输出为一个概率分布向量 $\sigma(z) &#x3D; [\sigma(z_1), \sigma(z_2), \dots, \sigma(z_n)]$，其中每个元素的计算公式如下：</p><p>$$<br>\sigma(z_i) &#x3D; \frac{e^{z_i}}{\sum_{j&#x3D;1}^{n} e^{z_j}}<br>$$</p><p>$$</p><ul><li><strong>分子</strong>：将每个元素 $z_i$映射为$e^{z_i}$。</li><li><strong>分母</strong>：是所有输入元素的指数和，确保输出的所有元素之和为 1。</li></ul><h3 id="1-4-交叉熵损失"><a href="#1-4-交叉熵损失" class="headerlink" title="1.4 交叉熵损失"></a>1.4 交叉熵损失</h3><p>交叉熵损失函数可化简为$\text{CrossEntropy} &#x3D; - \sum_{i&#x3D;1}^{N} \log(\hat{y}<em>{i, y_i})$,因为当且仅当y为真是才等于1，否则等于零，其完整公式为：$$\text{CrossEntropy} &#x3D; - \sum</em>{i&#x3D;1}^{N} \sum_{j&#x3D;1}^{C} y_{i,j} \cdot \log(\hat{y}_{i,j})$$</p><h2 id="2-一个例子"><a href="#2-一个例子" class="headerlink" title="2. 一个例子"></a>2. 一个例子</h2><h3 id="2-1-数据集"><a href="#2-1-数据集" class="headerlink" title="2.1 数据集"></a>2.1 数据集</h3><p>数据集来源于Fashion-mnist，主要通过softmax回归实现对若干图片的分类，例如：</p><p><img src="/minist.jpg" alt="部分数据展示"></p><h3 id="2-2-代码"><a href="#2-2-代码" class="headerlink" title="2.2 代码"></a>2.2 代码</h3><h4 id="2-2-0-读取数据集"><a href="#2-2-0-读取数据集" class="headerlink" title="2.2.0 读取数据集"></a>2.2.0 读取数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><p>将数据集读取到两个迭代器中，并把mini-batch的值设为256</p><h4 id="2-2-1-输出层定义"><a href="#2-2-1-输出层定义" class="headerlink" title="2.2.1 输出层定义"></a>2.2.1 输出层定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PytTorch不会隐式地调整输入的形状。因此，我们定义了展平层（flatten）在线性层前调整网络输入的形状</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="comment"># m为当前层</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="comment"># 使用均值为0、标准差为0.01的正态分布随机初始化权重</span></span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><ul><li><p>每个图片规格为28*28，因此先展平成784的向量。</p></li><li><p><code>nn.Linear(784, 10)</code>：这是一个全连接层（线性层），输入大小为 784，输出大小为 10，常用于将拉平后的数据映射到一个特定的输出空间（如 10 类分类任务）。</p></li><li><p><code>if type(m) == nn.Linear</code>：检查 <code>m</code> 是否为线性层。只有在该层为线性层的情况下，才进行权重初始化。</p></li><li><p><code>net.apply(init_weights)</code> 会遍历 <code>net</code> 中的所有层，并将每一层传入 <code>init_weights</code> 函数。这样只有线性层的权重会被初始化为指定的正态分布，而其他层（如 <code>nn.Flatten</code>）则保持不变。</p></li></ul><h4 id="2-2-2-交叉熵损失"><a href="#2-2-2-交叉熵损失" class="headerlink" title="2.2.2 交叉熵损失"></a>2.2.2 交叉熵损失</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交叉熵损失函数, nn.CrossEntropyLoss默认是对预测值做softmax,</span></span><br><span class="line"><span class="comment"># 所以我们的网络输出层不应用softmax</span></span><br><span class="line"><span class="comment"># 因为交叉熵损失函数的计算公式中已经包含了softmax运算</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><h4 id="2-2-3-优化算法"><a href="#2-2-3-优化算法" class="headerlink" title="2.2.3 优化算法"></a>2.2.3 优化算法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 优化算法, 使用小批量随机梯度下降, 学习率为0.1</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h4 id="2-2-4-训练"><a href="#2-2-4-训练" class="headerlink" title="2.2.4 训练"></a>2.2.4 训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">train(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch中的backward（）操作相关内容</title>
    <link href="/2024/10/26/pytorch%E4%B8%AD%E7%9A%84backward%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9/"/>
    <url>/2024/10/26/pytorch%E4%B8%AD%E7%9A%84backward%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9/</url>
    
    <content type="html"><![CDATA[<h2 id="1-PyTorch-利用-backward-求梯度的过程和原理"><a href="#1-PyTorch-利用-backward-求梯度的过程和原理" class="headerlink" title="1. PyTorch 利用 backward() 求梯度的过程和原理"></a>1. PyTorch 利用 <code>backward()</code> 求梯度的过程和原理</h2><p>在 PyTorch 中，<code>backward()</code> 是自动求导的关键函数，它用于计算张量的梯度。PyTorch 的自动求导功能基于“计算图”和“反向传播”原理，使得在训练深度学习模型时，计算损失函数相对于各参数的梯度变得高效便捷。下面介绍 PyTorch 中 <code>backward()</code> 求梯度的过程和原理。</p><h3 id="1-1-计算图（Computation-Graph）"><a href="#1-1-计算图（Computation-Graph）" class="headerlink" title="1.1 计算图（Computation Graph）"></a>1.1 计算图（Computation Graph）</h3><p>在 PyTorch 中，每个操作（如加法、乘法）都会创建一个计算节点，将这些节点连接起来形成<strong>计算图</strong>。这个图是有向无环图 (Directed Acyclic Graph, DAG)，从输入数据开始，一直到最终的输出。在计算图中：</p><ul><li>每个节点表示一个张量操作。</li><li>每条边表示操作之间的依赖关系。</li></ul><p>通过构建计算图，PyTorch 可以追踪到所有操作以及操作之间的依赖关系，为后续的反向传播提供了依据。</p><h3 id="1-2-反向传播（Backpropagation）"><a href="#1-2-反向传播（Backpropagation）" class="headerlink" title="1.2 反向传播（Backpropagation）"></a>1.2 反向传播（Backpropagation）</h3><p>反向传播是一种计算梯度的算法，利用链式法则，从输出层开始，反向逐层传播梯度。PyTorch 的 <code>backward()</code> 函数使用反向传播算法来自动计算梯度，具体过程如下：</p><ol><li><strong>前向传播</strong>：将输入数据经过网络，进行一系列操作得到最终输出和损失。</li><li><strong>计算损失的梯度</strong>：<ul><li>在调用 <code>backward()</code> 时，PyTorch 会从损失函数开始，沿计算图反向传播。</li><li>对于每一个张量的梯度 <code>∂L/∂x</code>（<code>L</code> 是损失函数，<code>x</code> 是张量），通过链式法则（即将每一步的梯度相乘）逐步计算梯度。</li></ul></li><li><strong>计算每层权重的梯度</strong>：将梯度传播至所有的叶子节点，即包含 <code>requires_grad=True</code> 的参数。此时，每个张量 <code>x</code> 的 <code>x.grad</code> 中会保存损失函数 <code>L</code> 对该张量的偏导数。</li></ol><h3 id="1-3-使用-backward-的步骤"><a href="#1-3-使用-backward-的步骤" class="headerlink" title="1.3 使用 backward() 的步骤"></a>1.3 使用 <code>backward()</code> 的步骤</h3><p>以下是 PyTorch 中使用 <code>backward()</code> 计算梯度的典型流程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建张量并启用梯度计算</span></span><br><span class="line">x = torch.tensor([<span class="number">2.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">z = y * <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设损失是z，计算梯度</span></span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># x.grad 包含 dz/dx 的值</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)  <span class="comment"># 输出: tensor([12.])</span></span><br></pre></td></tr></table></figure><h3 id="1-4-pytorch中梯度的累加"><a href="#1-4-pytorch中梯度的累加" class="headerlink" title="1.4 pytorch中梯度的累加"></a>1.4 pytorch中梯度的累加</h3><p>在 PyTorch 中，每个张量都有一个属性 <code>grad</code>，用来存储梯度值。当我们调用 <code>backward()</code> 时，计算的梯度值会被累加到该属性中。这意味着：</p><ul><li>多次调用 <code>backward()</code>，梯度会不断累加；</li><li>累加后的梯度值在优化步骤中被用于更新模型参数。</li></ul><p>通常情况下，如果每次梯度更新前不进行清零操作<code>optimizer.zero_grad()</code>，前一步计算的梯度将残留在 <code>grad</code> 属性中，影响下一步的梯度计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 model 是定义好的神经网络模型</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data, target <span class="keyword">in</span> dataloader:</span><br><span class="line">    optimizer.zero_grad()       <span class="comment"># 清零梯度</span></span><br><span class="line">    output = model(data)        <span class="comment"># 前向传播</span></span><br><span class="line">    loss = loss_fn(output, target)  <span class="comment"># 计算损失</span></span><br><span class="line">    loss.backward()             <span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">    optimizer.step()            <span class="comment"># 更新模型参数</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这里，<code>optimizer.zero_grad()</code> 确保每次反向传播前梯度清零，以免累加前一批次的数据产生的梯度。</p><h2 id="2-DataLoader"><a href="#2-DataLoader" class="headerlink" title="2.DataLoader"></a>2.DataLoader</h2><p>在 PyTorch 中，<code>DataLoader</code> 是用于加载数据的工具，可以帮助我们将数据按批次（batch）加载，并进行必要的预处理，如打乱数据、并行加载等。<code>DataLoader</code> 尤其适合在深度学习模型训练过程中高效地处理和管理数据。</p><h3 id="2-1-DataLoader-的作用"><a href="#2-1-DataLoader-的作用" class="headerlink" title="2.1 DataLoader 的作用"></a>2.1 DataLoader 的作用</h3><p><code>DataLoader</code> 的核心功能包括：</p><ul><li><p><strong>按批次加载数据</strong>：可以将数据集分为多个小批量（batch），使得每次训练迭代只用一个小批量数据，这对内存使用和优化效果有很大帮助。</p></li><li><p><strong>打乱数据</strong>：在每次 epoch 之前随机打乱数据的顺序，以提高模型的泛化能力。</p></li><li><p><strong>并行处理</strong>：通过多线程或多进程的方式加速数据加载，减少数据读取的时间消耗。</p></li></ul><h3 id="2-2-DataLoader-的基本用法"><a href="#2-2-DataLoader-的基本用法" class="headerlink" title="2.2 DataLoader 的基本用法"></a>2.2 DataLoader 的基本用法</h3><p>使用 <code>DataLoader</code> 的步骤主要有两个：</p><ol><li><strong>定义数据集</strong>：可以使用 PyTorch 内置的数据集（如 <code>torchvision.datasets</code>）或自定义数据集。</li><li><strong>创建 DataLoader 对象</strong>：将数据集传入 <code>DataLoader</code>，并设置批次大小和其他参数。</li></ol><p>以下是一个简单的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, TensorDataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建示例数据</span></span><br><span class="line">data = torch.arange(<span class="number">1</span>, <span class="number">11</span>).<span class="built_in">float</span>().view(-<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 10个样本，每个样本1维</span></span><br><span class="line">targets = data * <span class="number">2</span>  <span class="comment"># 假设目标是输入的2倍</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建TensorDataset</span></span><br><span class="line">dataset = TensorDataset(data, targets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用DataLoader加载数据</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代DataLoader</span></span><br><span class="line"><span class="keyword">for</span> batch_data, batch_targets <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Data:&quot;</span>, batch_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Targets:&quot;</span>, batch_targets)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="2-3-DataLoader-的参数说明"><a href="#2-3-DataLoader-的参数说明" class="headerlink" title="2.3 DataLoader 的参数说明"></a>2.3 DataLoader 的参数说明</h3><p><code>DataLoader</code> 的常用参数包括：</p><ul><li><strong>dataset</strong>：要加载的数据集对象。</li><li><strong>batch_size</strong>：每个批次的样本数量。</li><li><strong>shuffle</strong>：是否在每次迭代时打乱数据（一般在训练集上启用）。</li><li><strong>num_workers</strong>：用于加载数据的工作线程数量，通常在 GPU 训练时可以增加这个值以提高数据加载速度。</li><li><strong>drop_last</strong>：若 <code>True</code>，在数据大小不能整除 <code>batch_size</code> 时，丢弃最后一个不足批次的数据；若 <code>False</code> 则保留。</li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图片测试</title>
    <link href="/2024/10/26/%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95/"/>
    <url>/2024/10/26/%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<p><img src="/test2.jpg" alt="图片测试"></p>]]></content>
    
    
    
    <tags>
      
      <tag>测试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/10/26/hello-world/"/>
    <url>/2024/10/26/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="reate-a-new-post"><a href="#reate-a-new-post" class="headerlink" title="reate a new post"></a>reate a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>关于上传博客</title>
    <link href="/2024/10/26/%E5%85%B3%E4%BA%8E%E4%B8%8A%E4%BC%A0%E5%8D%9A%E5%AE%A2/"/>
    <url>/2024/10/26/%E5%85%B3%E4%BA%8E%E4%B8%8A%E4%BC%A0%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h2 id="上传博客命令"><a href="#上传博客命令" class="headerlink" title="上传博客命令"></a>上传博客命令</h2><h3 id="新建文章"><a href="#新建文章" class="headerlink" title="新建文章"></a>新建文章</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx hexo new “文章名称”</span><br></pre></td></tr></table></figure><h3 id="本地预览"><a href="#本地预览" class="headerlink" title="本地预览"></a>本地预览</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npx hexo g <span class="literal">-d</span></span><br><span class="line">npx hexo s</span><br></pre></td></tr></table></figure><h3 id="确认无误后先生成一编文件"><a href="#确认无误后先生成一编文件" class="headerlink" title="确认无误后先生成一编文件"></a>确认无误后先生成一编文件</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx hexo g</span><br></pre></td></tr></table></figure><h3 id="部署到Github"><a href="#部署到Github" class="headerlink" title="部署到Github"></a>部署到Github</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx hexo d</span><br></pre></td></tr></table></figure><h3 id="有关图片上传"><a href="#有关图片上传" class="headerlink" title="有关图片上传"></a>有关图片上传</h3><p>使用的是name.jpg，图片要放到同名目录下，并且本地编译后图片需要赋值到public中的相应目录，然后再提交，有时需多刷新几次。</p><p><img src="/test1.jpg" alt="图片引用方法"></p>]]></content>
    
    
    
    <tags>
      
      <tag>博客上传</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2024/10/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2024/10/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<p>这是一篇测试文章</p><p><img src="/test.jpg" alt="图片引用方法二"></p><p><img src="/test1.jpg" alt="图片引用方法三"></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
